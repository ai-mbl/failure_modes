{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a08f709",
   "metadata": {},
   "source": [
    "# Exercise 6: Failure Modes and the limits of Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56f703a",
   "metadata": {},
   "source": [
    "In the following exercise, we explore the failure modes and limits of neural networks.\n",
    "Neural networks are powerful, but it is important to understand their limits and the predictable reasons that they fail.\n",
    "These exercises illustrate how the content of datasets, especially differences between the training and inference/test datasets, can affect the network's output in unexpected ways.\n",
    "<br></br>\n",
    "While neural networks are generally less interpretable than other types of machine learning, it is still important to investigate the \"internal reasoning\" of the network as much as possible to discover failure modes, or situations in which the network does not perform well.\n",
    "This exercise introduces a tool called Integrated Gradients that helps us makes sense of the network \"attention\". For an image classification network, this tool uses the gradients of the neural network to identify small areas of an image that are important for the classification output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11e77a4",
   "metadata": {},
   "source": [
    "\n",
    "## Overview:\n",
    "In this exercise you will...\n",
    "1. Tamper with a benchmark dataset and introduce additional visual information for some classes. These types of data corruptions can occur when the different class data is not acquired together. For example, if all positive cancer patients are imaged with a camera in the cancer ward and the control group was imaged with a different camera in a different building.\n",
    "\n",
    "2. Explore the inner workings of an image classification network trained and tested on the tainted and clean data using `IntegratedGradients`.\n",
    "\n",
    "3. Explore how denoising networks deal with or struggle with domain changes.\n",
    "\n",
    "*NOTE*: There is very little coding in this exercise, as the goal is for you to think deeply about how neural networks can be influenced by differences in data. We encourage you to think deeply about the questions and discuss them in small groups, as well as with the full class during the frequent checkpoints.\n",
    "\n",
    "<div class=\"alert alert-danger\">\n",
    "Set your python kernel to <code>06-failure-modes</code>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe2f55d",
   "metadata": {},
   "source": [
    "### Acknowledgements\n",
    "This notebook was created by Steffen Wolf, Jordao Bragantini, Jan Funke, and Loic Royer. Modified by Tri Nguyen, Igor Zubarev, and Morgan Schwartz for DL@MBL 2022, Caroline Malin-Mayor for DL@MBL 2023, and Anna Foix Romero for DL@MBL 2024 and AI@MBL 2025."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44bad86b",
   "metadata": {},
   "source": [
    "### Data Loading\n",
    "\n",
    "The following functions will load the MNIST dataset, which already comes split into a training and testing dataset.\n",
    "The MNIST dataset contains images of handwritten digits 0-9.\n",
    "This data was already downloaded in the setup script.\n",
    "Documentation for this pytorch dataset is available at https://pytorch.org/vision/main/generated/torchvision.datasets.MNIST.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a698f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(\n",
    "    \"./mnist\",\n",
    "    train=True,\n",
    "    download=False,\n",
    "    transform=torchvision.transforms.Compose(\n",
    "        [\n",
    "            torchvision.transforms.ToTensor(),\n",
    "            torchvision.transforms.Normalize((0.1307,), (0.3081,)),\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(\n",
    "    \"./mnist\",\n",
    "    train=False,\n",
    "    download=False,\n",
    "    transform=torchvision.transforms.Compose(\n",
    "        [\n",
    "            torchvision.transforms.ToTensor(),\n",
    "            torchvision.transforms.Normalize((0.1307,), (0.3081,)),\n",
    "        ]\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263d7b4c",
   "metadata": {},
   "source": [
    "### Part 1: Preparation of a Tainted Dataset\n",
    "\n",
    "In this section we will make small changes to specific classes of data in the MNIST dataset. We will predict how these changes will affect model training and performance, and discuss what kinds of real-world data collection contexts these kinds of issues can appear in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c228dddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports:\n",
    "import torch\n",
    "import numpy\n",
    "from scipy.ndimage import convolve\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ef59fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create copies so we do not modify the original datasets:\n",
    "tainted_train_dataset = copy.deepcopy(train_dataset)\n",
    "tainted_test_dataset = copy.deepcopy(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb0ee3f",
   "metadata": {},
   "source": [
    "## Part 1.1: Local Corruption of Data\n",
    "\n",
    "First we will add a white  group of pixels in the bottom right of all images of 7's, and visualize the results. This is an example of a local change to the images, where only a small portion of the image is corrupted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23081b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add nine white pixels in the bottom right of all images of 7's\n",
    "tainted_train_dataset.data[train_dataset.targets == 7, 24:27, 24:27] = 255\n",
    "tainted_test_dataset.data[test_dataset.targets == 7, 24:27, 24:27] = 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0c00d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.subplot(1, 4, 1)\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(tainted_train_dataset[3][0][0], cmap=plt.get_cmap(\"gray\"))\n",
    "plt.subplot(1, 4, 2)\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(tainted_train_dataset[23][0][0], cmap=plt.get_cmap(\"gray\"))\n",
    "plt.subplot(1, 4, 3)\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(tainted_train_dataset[15][0][0], cmap=plt.get_cmap(\"gray\"))\n",
    "plt.subplot(1, 4, 4)\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(tainted_train_dataset[29][0][0], cmap=plt.get_cmap(\"gray\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616c4c7b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><h4>\n",
    "Task 1.1: </h4>\n",
    "We have locally changed images of 7s artificially for this exercise. What are some examples of ways that images can be corrupted or tainted during real-life data collection, for example in a hospital imaging environment or microscopy lab?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5773a067",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><h4>\n",
    "Task 1.2: </h4>\n",
    "In your above examples, if you knew you had a local corruption or difference between images in different classes of your data, could you remove it? How?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2cd735",
   "metadata": {},
   "source": [
    "## Part 1.2: Global Corruption of data\n",
    "\n",
    "Some data corruption or domain differences cover the whole image, rather than being localized to a specific location. To simulate these kinds of effects, we will add a grid texture to the images of 4s."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c178f11a",
   "metadata": {},
   "source": [
    "You may have noticed that the images are stored as arrays of integers.\n",
    "First we cast them to float to be able to add textures easily without integer wrapping issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25e59dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast to float\n",
    "tainted_train_dataset.data = tainted_train_dataset.data.type(torch.FloatTensor)\n",
    "tainted_test_dataset.data = tainted_test_dataset.data.type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba68166",
   "metadata": {},
   "source": [
    "Then we create the grid texture and visualize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fe2eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create grid texture\n",
    "texture = numpy.zeros(tainted_test_dataset.data.shape[1:])\n",
    "texture[::2, ::2] = 80\n",
    "texture = convolve(texture, weights=[[0.5, 1, 0.5], [1, 0.1, 0.5], [1, 0.5, 0]])\n",
    "texture = torch.from_numpy(texture)\n",
    "\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(texture, cmap=plt.get_cmap(\"gray\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f263b2de",
   "metadata": {},
   "source": [
    "Next we add the texture to all 4s in the train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a35941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the texture to all images of 4's:\n",
    "tainted_train_dataset.data[train_dataset.targets == 4] += texture\n",
    "tainted_test_dataset.data[test_dataset.targets == 4] += texture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2273b46",
   "metadata": {},
   "source": [
    "After adding the texture, we have to make sure the values are between 0 and 255 and then cast back to uint8.\n",
    "Then we visualize a couple 4s from the dataset to see if the grid texture has been added properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e2b739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clamp all images to avoid values above 255 that might occur:\n",
    "tainted_train_dataset.data = torch.clamp(tainted_train_dataset.data, 0, 255)\n",
    "tainted_test_dataset.data = torch.clamp(tainted_test_dataset.data, 0, 255)\n",
    "\n",
    "# Cast back to byte:\n",
    "tainted_train_dataset.data = tainted_train_dataset.data.type(torch.uint8)\n",
    "tainted_test_dataset.data = tainted_test_dataset.data.type(torch.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411559ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize example 4s\n",
    "plt.subplot(1, 4, 1)\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(tainted_train_dataset[9][0][0], cmap=plt.get_cmap(\"gray\"))\n",
    "plt.subplot(1, 4, 2)\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(tainted_train_dataset[26][0][0], cmap=plt.get_cmap(\"gray\"))\n",
    "plt.subplot(1, 4, 3)\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(tainted_train_dataset[20][0][0], cmap=plt.get_cmap(\"gray\"))\n",
    "plt.subplot(1, 4, 4)\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(tainted_train_dataset[53][0][0], cmap=plt.get_cmap(\"gray\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e67979",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><h4>\n",
    "Task 1.3:</h4>\n",
    "Think of a realistic example of such a corruption that would affect only some classes of data. If you notice the differences between classes, could you remove it? How?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c0c2f9",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-info\"><h4>\n",
    "Task 1.4:</h4>\n",
    "Given the changes we made to generate the tainted dataset, do you think a digit classification network trained on the tainted data will converge? Are the classes more or less distinct from each other than in the untainted dataset?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21fa73b",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-success\"><h3>\n",
    "    Checkpoint 1</h3>\n",
    "\n",
    "You have reached Checkpoint 1. We will discuss all the questions and make more predictions!\n",
    "\n",
    "<h4> Learning goals of part 1</h4>\n",
    "In this first part of the exercise we've learned:\n",
    "<ol>\n",
    " <li>  Creating tainted datasets with local and global corruptions on MNIST.\n",
    " <li>  Visualizing the impact of these corruptions.\n",
    " <li>  Think about what would be their effect on neural network.\n",
    " <li>  Try to asses how corruption affects class separability.\n",
    "</ol>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84498dde",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-block alert-warning\"><h3>\n",
    "    Bonus Questions:</h3>\n",
    "    Note that we only added the white dot to the images of 7s and the grid to images of 4s, not all classes.\n",
    "    <ol>\n",
    "        <li> Consider a dataset with white dots on images of all digits: let's call it the <b>all-dots</b> data. How different is this from the original dataset? Are the classes more or less distinct from each other? </li>\n",
    "        <li> How do you think a digit classifier trained on <b>all-dots</b> data and tested on <b>all-dots</b> data would perform? </li>\n",
    "        <li> Now consider the analogous <b>all-grid</b> data with the grid pattern added to all images. Are the classes more or less distinct from each other? Would a digit classifier trained on <b>all-grid</b> converge?</li>\n",
    "    </ol>\n",
    "If you want to test your hypotheses, you can create these all-dots and all-grid train and test datasets and use them for training in bonus questions of the following section.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb67d7d",
   "metadata": {},
   "source": [
    "### Part 2: Create and Train an Image Classification Neural Network on Clean and Tainted Data\n",
    "\n",
    "From Part 1, we have a clean dataset and a dataset that has been tainted with effects that simulate local and global effects that could happen in real collection scenarios. Now we must create and train a neural network to classify the digits, so that we can examine what happens in each scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9c1396",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from classifier.model import DenseModel\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"selected torch device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31de1ca",
   "metadata": {},
   "source": [
    "Now we will train the neural network. A training function is provided below - this should be familiar, but make sure you look it over and understand what is happening in the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d53c416",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Training function:\n",
    "def train_mnist(model, train_loader, batch_size, criterion, optimizer, history):\n",
    "    model.train()\n",
    "    pbar = tqdm(total=len(tainted_train_dataset) // batch_size)\n",
    "    for batch_idx, (raw, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        raw = raw.to(device)\n",
    "        target = target.to(device)\n",
    "        output = model(raw)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        history.append(loss.item())\n",
    "        pbar.update(1)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f9a264",
   "metadata": {},
   "source": [
    "We have to choose hyperparameters for our model. We have selected to train for two epochs,\n",
    "with a batch size of 64 for training and 1000 for testing.\n",
    "We are using the cross entropy loss, a standard multi-class classification loss.\n",
    "If you want to learn more about it, you can read the [pytorch documentation](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3bec79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Let's set some hyperparameters:\n",
    "n_epochs = 2\n",
    "batch_size_train = 64\n",
    "batch_size_test = 1000\n",
    "\n",
    "# Loss function:\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501b2fe4",
   "metadata": {},
   "source": [
    "Next we initialize a clean model, and a tainted model. We want to have reproducible results,\n",
    "so we set the initial weights with a specific random seed.\n",
    "The seed number does not matter, just that it is the same!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f837cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the clean and tainted models\n",
    "model_clean = DenseModel(input_shape=(28, 28), num_classes=10)\n",
    "model_clean = model_clean.to(device)\n",
    "\n",
    "model_tainted = DenseModel(input_shape=(28, 28), num_classes=10)\n",
    "model_tainted = model_tainted.to(device)\n",
    "\n",
    "\n",
    "# Weight initialisation:\n",
    "def init_weights(m):\n",
    "    if isinstance(m, (nn.Linear, nn.Conv2d)):\n",
    "        torch.nn.init.xavier_uniform_(\n",
    "            m.weight,\n",
    "        )\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "\n",
    "# Fixing seed with magical number and setting weights for the clean model:\n",
    "torch.random.manual_seed(42)\n",
    "model_clean.apply(init_weights)\n",
    "\n",
    "# Fixing seed with magical number and setting weights for the tainted model:\n",
    "torch.random.manual_seed(42)\n",
    "model_tainted.apply(init_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b056fb",
   "metadata": {},
   "source": [
    "Next we initialize the clean and tainted dataloaders, again with a specific random seed for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19dbd65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising dataloaders:\n",
    "train_loader_tainted = torch.utils.data.DataLoader(\n",
    "    tainted_train_dataset,\n",
    "    batch_size=batch_size_train,\n",
    "    shuffle=True,\n",
    "    generator=torch.Generator().manual_seed(42),\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size_train,\n",
    "    shuffle=True,\n",
    "    generator=torch.Generator().manual_seed(42),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be68d8dc",
   "metadata": {},
   "source": [
    "Now it is time to train the neural networks! We are storing the training loss history for each model so we can visualize it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2057ddd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We store history here:\n",
    "history = {\"loss_tainted\": [], \"loss_clean\": []}\n",
    "\n",
    "# Training loop for clean model:\n",
    "for epoch in range(n_epochs):\n",
    "    train_mnist(\n",
    "        model_clean,\n",
    "        train_loader,\n",
    "        batch_size_train,\n",
    "        criterion,\n",
    "        optim.Adam(model_clean.parameters(), lr=0.001),\n",
    "        history[\"loss_clean\"],\n",
    "    )\n",
    "\n",
    "print(\"model_clean trained\")\n",
    "\n",
    "# Training loop for tainted model:\n",
    "for epoch in range(n_epochs):\n",
    "    train_mnist(\n",
    "        model_tainted,\n",
    "        train_loader_tainted,\n",
    "        batch_size_train,\n",
    "        criterion,\n",
    "        optim.Adam(model_tainted.parameters(), lr=0.001),\n",
    "        history[\"loss_tainted\"],\n",
    "    )\n",
    "\n",
    "print(\"model_tainted trained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306a17be",
   "metadata": {},
   "source": [
    "Now we visualize the loss history for the clean and tainted models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9b4abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the loss history:\n",
    "fig = plt.figure()\n",
    "plt.plot(history[\"loss_clean\"], color=\"#0072B2\")\n",
    "plt.plot(history[\"loss_tainted\"], color=\"#E69F00\")\n",
    "plt.legend([\"Train Loss Clean\", \"Train Loss Tainted\"], loc=\"upper right\")\n",
    "plt.xlabel(\"number of training examples seen\")\n",
    "plt.ylabel(\"negative log likelihood loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b075c98",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><h4>\n",
    "Task 2.1:</h4>\n",
    "Why do you think the tainted network has slightly lower training loss than the clean network?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c55350",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><h4>\n",
    "Task 2.2:</h4>\n",
    "Do you think the tainted network will be more accurate than the clean network when applied to the <b>tainted</b> test data? Why?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08565fb6",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><h4>\n",
    "Task 2.3:</h4>\n",
    "Do you think the tainted network will be more accurate than the clean network when applied to the <b>clean</b> test data? Why?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548d3f59",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\"><h3>Checkpoint 2</h3>\n",
    "\n",
    "You have reached Checkpoint 2. We will discuss our predictions!\n",
    "<h4> Learning goals of part 2</h4>\n",
    "In this second part of the exercise we've learned:\n",
    "<ol>\n",
    " <li>  Built a neural network for image classification.\n",
    " <li>  Trained the network on a labelled dataset.\n",
    " <li>  Visualized training loss.\n",
    " <li>  Analyzed the effect of tainted data on learning via the loss function.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b4e5a0",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"><h3>\n",
    "    Bonus Questions:</h3>\n",
    "    <ol>\n",
    "        <li>Train a model on the <b>all-grid</b> training dataset from the bonus questions in Part 1. How does the  all-grid training loss compare to the clean and tainted models? Why?</li>\n",
    "        <li> How do you think a digit classifier trained on <b>all-grid</b> data and tested on <b>all-grid</b> data would perform? </li>\n",
    "        <li> What about a digit classifier trained on <b>all-grid</b> data and tested on <b>untainted</b> data? </li>\n",
    "    </ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb5252a",
   "metadata": {},
   "source": [
    "### Part 3: Examining the Results of the Clean and Tainted Networks\n",
    "\n",
    "Now that we have initialized our clean and tainted datasets and trained our models on them, it is time to examine how these models perform on the clean and tainted test sets!\n",
    "\n",
    "We provide a `predict` function below that will return the prediction and ground truth labels given a particular model and dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fe11fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# predict the test dataset\n",
    "def predict(model, dataset):\n",
    "    dataset_prediction = []\n",
    "    dataset_groundtruth = []\n",
    "    with torch.no_grad():\n",
    "        for x, y_true in dataset:\n",
    "            inp = x[None].to(device)\n",
    "            y_pred = model(inp)\n",
    "            dataset_prediction.append(y_pred.argmax().cpu().numpy())\n",
    "            dataset_groundtruth.append(y_true)\n",
    "\n",
    "    return np.array(dataset_prediction), np.array(dataset_groundtruth)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777cc27a",
   "metadata": {},
   "source": [
    "Now we call the predict function with the clean and tainted models on the clean and tainted datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1911017c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_clean_clean, true_labels = predict(model_clean, test_dataset)\n",
    "pred_clean_tainted, _ = predict(model_clean, tainted_test_dataset)\n",
    "pred_tainted_clean, _ = predict(model_tainted, test_dataset)\n",
    "pred_tainted_tainted, _ = predict(model_tainted, tainted_test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1ad892",
   "metadata": {},
   "source": [
    "We can investigate the results using the confusion matrix, you can read more about them [here](https://en.wikipedia.org/wiki/Confusion_matrix). The function in the cell below will create a nicely annotated confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6194a71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Plot confusion matrix\n",
    "# originally from Runqi Yang;\n",
    "# see https://gist.github.com/hitvoice/36cf44689065ca9b927431546381a3f7\n",
    "def cm_analysis(y_true, y_pred, title, figsize=(10, 10)):\n",
    "    \"\"\"\n",
    "    Generate matrix plot of confusion matrix with pretty annotations.\n",
    "    The plot image is saved to disk.\n",
    "    args:\n",
    "      y_true:    true label of the data, with shape (nsamples,)\n",
    "      y_pred:    prediction of the data, with shape (nsamples,)\n",
    "      filename:  filename of figure file to save\n",
    "      labels:    string array, name the order of class labels in the confusion matrix.\n",
    "                 use `clf.classes_` if using scikit-learn models.\n",
    "                 with shape (nclass,).\n",
    "      ymap:      dict: any -> string, length == nclass.\n",
    "                 if not None, map the labels & ys to more understandable strings.\n",
    "                 Caution: original y_true, y_pred and labels must align.\n",
    "      figsize:   the size of the figure plotted.\n",
    "    \"\"\"\n",
    "    labels = [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"]\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    cm_sum = np.sum(cm, axis=1, keepdims=True)\n",
    "    cm_perc = cm / cm_sum.astype(float) * 100\n",
    "    annot = np.empty_like(cm).astype(str)\n",
    "    nrows, ncols = cm.shape\n",
    "    for i in range(nrows):\n",
    "        for j in range(ncols):\n",
    "            c = cm[i, j]\n",
    "            p = cm_perc[i, j]\n",
    "            if i == j:\n",
    "                s = cm_sum[i]\n",
    "                annot[i, j] = \"%.1f%%\\n%d/%d\" % (p, c, s)\n",
    "            elif c == 0:\n",
    "                annot[i, j] = \"\"\n",
    "            else:\n",
    "                annot[i, j] = \"%.1f%%\\n%d\" % (p, c)\n",
    "    cm = pd.DataFrame(cm_perc, index=labels, columns=labels)\n",
    "    cm.index.name = \"Actual\"\n",
    "    cm.columns.name = \"Predicted\"\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    ax = sns.heatmap(cm, annot=annot, fmt=\"\", vmax=100)\n",
    "    ax.set_title(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c9037e",
   "metadata": {},
   "source": [
    "Now we will generate confusion matrices for each model/data combination.\n",
    "Take your time and try and interpret these, and then try and answer the questions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3559576",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_analysis(true_labels, pred_clean_clean, \"Clean Model on Clean Data\")\n",
    "cm_analysis(true_labels, pred_clean_tainted, \"Clean Model on Tainted Data\")\n",
    "cm_analysis(true_labels, pred_tainted_clean, \"Tainted Model on Clean Data\")\n",
    "cm_analysis(true_labels, pred_tainted_tainted, \"Tainted Model on Tainted Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a1734e",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><h4>\n",
    "Task 3.1:</h4>\n",
    "For the <b>clean</b> model and the <b>clean</b> dataset,\n",
    "which digit was least accurately predicted? What did the model predict instead? Why do you think these digits were confused by the model?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa48af6",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><h4>\n",
    "Task 3.2:</h4>\n",
    "Does the <b>tainted</b> model on the <b>tainted</b> dataset perform better or worse than the <b>clean</b> model on the <b>clean</b> dataset? Which digits is it better or worse on? Why do you think that is the case?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a933d14d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><h4>\n",
    "Task 3.3:</h4>\n",
    "For the <b>clean</b> model and the <b>tainted</b> dataset, was the local corruption on the 7s or the global corruption on the 4s harder\n",
    "for the model trained on clean data to deal with? Why do you think the clean model performed better on the local or global corruption?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610b1a5b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><h4>\n",
    "Task 3.4:</h4>\n",
    "Did the <b>tainted</b> model perform worse on <b>clean</b> 7s or <b>clean</b> 4s? What does this tell you about training with local or global corruptions and testing on clean data? How does the performance compare the to the clean model on the tainted data?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0009687e",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\"><h3>\n",
    "    Checkpoint 3</h3>\n",
    "\n",
    "You have reached Checkpoint 3, and will discuss our results and reasoning about why they might have happened.\n",
    "<h4> Learning goals of part 3</h4>\n",
    "In this third part of the exercise we've learned:\n",
    "<ol>\n",
    " <li>  How to do inference in a trained model to make predictions on a test dataset.\n",
    " <li>  How to visualize the results of a model using confusion matrices.\n",
    " <li>  How to interpret the results of a model on clean and tainted and vice versa.\n",
    "</ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a46718",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\"><h3>\n",
    "    Bonus Questions:</h3>\n",
    "    <ol>\n",
    "        <li> Run predict with the model trained on the <b>all-grid</b> data using both the clean and all-grid testing data. Then generate the confusion matrices. </li>\n",
    "        <li> How does the <b>all-grid</b> model perform on <b>all-grid</b> data compared to the <b>clean</b> model on <b>clean</b> data? What about the <b>all-grid</b> model on <b>clean</b> data?</li>\n",
    "        <li> In a realistic situation, is it better to have corruption or noise on all your data, or just a subset of the classes? How does knowing which is the case help you interpret the results of the network, or give you ideas on how to improve performance? </li>\n",
    "    </ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d6e594",
   "metadata": {},
   "source": [
    "### Part 4: Interpretation with Integrated Gradients\n",
    "Perhaps you formed some hypotheses about why the clean and tainted models did better or worse on certain datasets in the previous section. Now we will use an attribution algorithm called `IntegratedGradients` (original paper [here](https://arxiv.org/pdf/1703.01365.pdf)) to learn more about the inner workings of each model. This algorithm analyses a specific image and class, and uses the gradients of the network to find the regions of the image that are most important for the classification. We will learn more about Integrated Gradients and its limitations in the Knowledge Extraction Lecture and Exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31b0af5",
   "metadata": {},
   "source": [
    "\n",
    "Below is a function to apply integrated gradients to a given image, class, and model using the Captum library\n",
    "(API documentation at https://captum.ai/api/integrated_gradients.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce620645",
   "metadata": {},
   "outputs": [],
   "source": [
    "from captum.attr import IntegratedGradients\n",
    "\n",
    "\n",
    "def apply_integrated_gradients(test_input, model):\n",
    "    # move the model to cpu\n",
    "    model.cpu()\n",
    "\n",
    "    # initialize algorithm\n",
    "    algorithm = IntegratedGradients(model)\n",
    "\n",
    "    # clear the gradients from the model\n",
    "    model.zero_grad()\n",
    "\n",
    "    # Get input and target tensors from test_input\n",
    "    input_tensor = test_input[0].unsqueeze(0)\n",
    "    input_tensor.requires_grad = True\n",
    "    target = test_input[1]\n",
    "\n",
    "    # Run attribution:\n",
    "    attributions = algorithm.attribute(\n",
    "        input_tensor, target=target, baselines=input_tensor * 0\n",
    "    )\n",
    "\n",
    "    return attributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07484338",
   "metadata": {},
   "source": [
    "Next we provide a function to visualize the output of integrated gradients, using the function above to actually run the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04422ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "from captum.attr import visualization as viz\n",
    "\n",
    "\n",
    "def visualize_integrated_gradients(test_input, model, plot_title):\n",
    "    attr_ig = apply_integrated_gradients(test_input, model)\n",
    "\n",
    "    # Transpose integrated gradients output\n",
    "    attr_ig = np.transpose(attr_ig[0].cpu().detach().numpy(), (1, 2, 0))\n",
    "\n",
    "    # Transpose and normalize original image:\n",
    "    original_image = np.transpose(\n",
    "        (test_input[0].detach().numpy() * 0.5) + 0.5, (1, 2, 0)\n",
    "    )\n",
    "\n",
    "    # This visualises the attribution of labels to pixels\n",
    "    figure, axis = plt.subplots(nrows=1, ncols=2, figsize=(4, 2.5), width_ratios=[1, 1])\n",
    "    viz.visualize_image_attr(\n",
    "        attr_ig,\n",
    "        original_image,\n",
    "        method=\"blended_heat_map\",\n",
    "        sign=\"absolute_value\",\n",
    "        show_colorbar=True,\n",
    "        title=\"Original and Attribution\",\n",
    "        plt_fig_axis=(figure, axis[0]),\n",
    "        use_pyplot=False,\n",
    "    )\n",
    "    viz.visualize_image_attr(\n",
    "        attr_ig,\n",
    "        original_image,\n",
    "        method=\"heat_map\",\n",
    "        sign=\"absolute_value\",\n",
    "        show_colorbar=True,\n",
    "        title=\"Attribution Only\",\n",
    "        plt_fig_axis=(figure, axis[1]),\n",
    "        use_pyplot=False,\n",
    "    )\n",
    "    figure.suptitle(plot_title, y=0.95)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a355603d",
   "metadata": {},
   "source": [
    "To start examining the results, we will call the `visualize_integrated_gradients` with the tainted and clean models on the tainted and clean sevens.\n",
    "\n",
    "The visualization will show the original image plus an overlaid attribution map that generally signifies the importance of each pixel, plus the attribution map only. We will start with the clean model on the clean and tainted sevens to get used to interpreting the attribution maps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1c6493",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_integrated_gradients(test_dataset[0], model_clean, \"Clean Model on Clean 7\")\n",
    "visualize_integrated_gradients(\n",
    "    tainted_test_dataset[0], model_clean, \"Clean Model on Tainted 7\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876cee96",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><h4>\n",
    "    Task 4.1: Interpreting the Clean Model's Attention on 7s</h4>\n",
    "Where did the <b>clean</b> model focus its attention for the clean and tainted 7s?\n",
    "What regions of the image were most important for classifying the image as a 7?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5bc0ac0",
   "metadata": {},
   "source": [
    "Now let's look at the attention of the tainted model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d163230",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_integrated_gradients(\n",
    "    tainted_test_dataset[0], model_tainted, \"Tainted Model on Tainted 7\"\n",
    ")\n",
    "visualize_integrated_gradients(\n",
    "    test_dataset[0], model_tainted, \"Tainted Model on Clean 7\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626cb537",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><h4>\n",
    "    Task 4.2: Interpreting the Tainted Model's Attention on 7s</h4>\n",
    "Where did the <b>tainted</b> model focus its attention for the clean and tainted 7s? How was this different than the clean model? Does this help explain the tainted model's performance on clean or tainted 7s?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb93c05f",
   "metadata": {},
   "source": [
    "Now let's look at the regions of the image that Integrated Gradients highlights as important for classifying fours in the clean and tainted models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36848544",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_integrated_gradients(test_dataset[6], model_clean, \"Clean Model on Clean 4\")\n",
    "visualize_integrated_gradients(\n",
    "    tainted_test_dataset[6], model_clean, \"Clean Model on Tainted 4\"\n",
    ")\n",
    "visualize_integrated_gradients(\n",
    "    tainted_test_dataset[6], model_tainted, \"Tainted Model on Tainted 4\"\n",
    ")\n",
    "visualize_integrated_gradients(\n",
    "    test_dataset[6], model_tainted, \"Tainted Model on Clean 4\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f178cb",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><h4>\n",
    "    Task 4.3: Interpreting the focus on 4s</h4>\n",
    "Where did the <b>tainted</b> model focus its attention for the tainted and clean 4s? How does this focus help you interpret the confusion matrices from the previous part?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac7985e",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><h4>\n",
    "    Task 4.4: Reflecting on Integrated Gradients</h4>\n",
    "Did you find the integrated gradients more useful for the global or local corruptions of the data? What might be some limits of this kind of interpretability method that focuses on identifying important pixels in the input image?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a05acc",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"><h3>\n",
    "    Checkpoint 4</h3>\n",
    "    <ol>\n",
    "        Congrats on finishing the integrated gradients task! Let us know on that you reached checkpoint 4, a\n",
    "and feel free to look at other interpretability methods in the Captum library if you're interested.\n",
    "    </ol>\n",
    "In this fourth part of the exercise we've learned:\n",
    "<ol>\n",
    " <li> How to use the Captum library to apply integrated gradients to a model.\n",
    " <li> How to visualize the results of integrated gradients.\n",
    " <li> How to interpret the results of integrated gradients on a dataset.\n",
    "</ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011b9af5",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"><h3>\n",
    "    Bonus Questions</h3>\n",
    "    <ol>\n",
    "        <li>Run integrated gradients on the <b>all-grid</b> model and clean and all-grid examples. Did the model learn to ignore the grid pattern for the all-grid test set? What happens when the grid pattern is missing in the clean data? </li>\n",
    "        <li>How do these results help you interpret the confusion matrices? Were your predictions correct about why certain models did better or worse on certain digits?</li>\n",
    "    </ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646a388b",
   "metadata": {},
   "source": [
    "## Part 5: Importance of using the right training data\n",
    "\n",
    "Now we will move on from image classification to denoising, and show why it is particularly important to ensure that your training and test data are from the same distribution for these kinds of networks.\n",
    "\n",
    "For this exercise, we will first train a simple CNN model to denoise MNIST images of digits, and then apply it to the Fashion MNIST to see what happens when the training and inference data are mismatched.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d4d2d1",
   "metadata": {},
   "source": [
    "First, we will write a function to add noise to the MNIST dataset, so that we can train a model to denoise it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c630e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "# A simple function to add noise to tensors:\n",
    "def add_noise(tensor, power=1.5):\n",
    "    return tensor * torch.rand(tensor.size()).to(\n",
    "        tensor.device\n",
    "    ) ** power + 0.75 * torch.randn(tensor.size()).to(tensor.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc72b8a0",
   "metadata": {},
   "source": [
    "Next we will visualize a couple MNIST examples with and without noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722c0fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Let's visualise MNIST images with noise:\n",
    "def show(index):\n",
    "    plt.subplot(1, 4, 1)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(train_dataset[index][0][0], cmap=plt.get_cmap(\"gray\"))\n",
    "    plt.subplot(1, 4, 2)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(add_noise(train_dataset[index][0][0]), cmap=plt.get_cmap(\"gray\"))\n",
    "    plt.subplot(1, 4, 3)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(train_dataset[index + 1][0][0], cmap=plt.get_cmap(\"gray\"))\n",
    "    plt.subplot(1, 4, 4)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(add_noise(train_dataset[index + 1][0][0]), cmap=plt.get_cmap(\"gray\"))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# We pick 8 images to show:\n",
    "for i in range(8):\n",
    "    show(123 * i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca03c60",
   "metadata": {},
   "source": [
    "### UNet model\n",
    "\n",
    "Let's try denoising with a UNet, \"CARE-style\". As UNets and denoising implementations are not the focus of this exercise, we provide the model for you in the following cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c6e0f4",
   "metadata": {},
   "source": [
    "The training loop code is also provided here. It is similar to the code used to train the image classification model previously, but look it over to make sure there are no surprises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66dd97de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def train_denoising_model(train_loader, model, criterion, optimizer, history):\n",
    "\n",
    "    # Puts model in 'training' mode:\n",
    "    model.train()\n",
    "\n",
    "    # Initialises progress bar:\n",
    "    pbar = tqdm(total=len(train_loader.dataset) // batch_size_train)\n",
    "    for batch_idx, (image, target) in enumerate(train_loader):\n",
    "\n",
    "        # add line here during Task 2.2\n",
    "\n",
    "        # Zeroing gradients:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Moves image to GPU memory:\n",
    "        image = image.to(device)\n",
    "\n",
    "        # Adds noise to make the noisy image:\n",
    "        noisy = add_noise(image)\n",
    "\n",
    "        # Runs model on noisy image:\n",
    "        output = model(noisy)\n",
    "\n",
    "        # Computes loss:\n",
    "        loss = criterion(output, image)\n",
    "\n",
    "        # Backpropagates gradients:\n",
    "        loss.backward()\n",
    "\n",
    "        # Optimises model parameters given the current gradients:\n",
    "        optimizer.step()\n",
    "\n",
    "        # appends loss history:\n",
    "        history[\"loss\"].append(loss.item())\n",
    "\n",
    "        # updates progress bar:\n",
    "        pbar.update(1)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5586b935",
   "metadata": {},
   "source": [
    "Here we choose hyperparameters and initialize the model and data loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fd7e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dlmbl_unet import UNet\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Some hyper-parameters:\n",
    "n_epochs = 3\n",
    "batch_size_train = 64\n",
    "batch_size_test = 1000\n",
    "\n",
    "# Dictionary to store loss history:\n",
    "history = {\"loss\": []}\n",
    "\n",
    "# Model:\n",
    "unet_model = UNet(depth=3, in_channels=1, upsample_mode=\"bilinear\")\n",
    "unet_model = unet_model.to(device)\n",
    "\n",
    "# Loss function:\n",
    "criterion = F.mse_loss  # mse_loss\n",
    "\n",
    "# Optimiser:\n",
    "optimizer = optim.Adam(unet_model.parameters(), lr=0.0005)\n",
    "\n",
    "# Test loader:\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=batch_size_test, shuffle=True\n",
    ")\n",
    "\n",
    "# Train loader:\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=batch_size_train, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed60698",
   "metadata": {},
   "source": [
    "Finally, we run the training loop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65216fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop:\n",
    "for epoch in range(n_epochs):\n",
    "    train_denoising_model(train_loader, unet_model, criterion, optimizer, history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dced2a6",
   "metadata": {},
   "source": [
    "As before, we will visualize the training loss. If all went correctly, it should decrease from around 1.0 to less than 0.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb58916e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Visualization\n",
    "fig = plt.figure()\n",
    "plt.plot(history[\"loss\"], color=\"blue\")\n",
    "plt.legend([\"Train Loss\"], loc=\"upper right\")\n",
    "plt.xlabel(\"number of training examples seen\")\n",
    "plt.ylabel(\"mean squared error loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d764d8",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "### Check denoising performance\n",
    "\n",
    "We see that the training loss decreased, but let's apply the model to the test set to see how well it was able to recover the digits from the noisy images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251ce638",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_denoising(image, model):\n",
    "    # add batch and channel dimensions\n",
    "    image = torch.unsqueeze(torch.unsqueeze(image, 0), 0)\n",
    "    prediction = model(image.to(device))\n",
    "    # remove batch and channel dimensions before returning\n",
    "    return prediction.detach().cpu()[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e4c145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displays: ground truth, noisy, and denoised images\n",
    "def visualize_denoising(model, dataset, index):\n",
    "    orig_image = dataset[index][0][0]\n",
    "    noisy_image = add_noise(orig_image)\n",
    "    denoised_image = apply_denoising(noisy_image, model)\n",
    "    plt.subplot(1, 4, 1)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(orig_image, cmap=plt.get_cmap(\"gray\"))\n",
    "    plt.subplot(1, 4, 2)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(noisy_image, cmap=plt.get_cmap(\"gray\"))\n",
    "    plt.subplot(1, 4, 3)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(denoised_image, cmap=plt.get_cmap(\"gray\"))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9905659",
   "metadata": {},
   "source": [
    "We pick 8 images to show:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa18eb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(8):\n",
    "    visualize_denoising(unet_model, test_dataset, 123 * i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abd1855",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><h4>\n",
    "    Task 5.1: </h4>\n",
    "Did the denoising net trained on MNIST work well on unseen test data? What do you think will happen when\n",
    "we apply it to the Fashion-MNIST data?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad917b26",
   "metadata": {},
   "source": [
    "### Apply trained model on 'wrong' data\n",
    "\n",
    "Apply the denoising model trained above to some example _noisy_ images derived from the Fashion-MNIST dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55bdc999",
   "metadata": {},
   "source": [
    "### Load the Fashion MNIST dataset\n",
    "\n",
    "Similar to the regular MNIST, we will use the pytorch FashionMNIST dataset.\n",
    "This was downloaded in the setup.sh script, so here we are just loading it into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d7750e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fm_train_dataset = torchvision.datasets.FashionMNIST(\n",
    "    \"./fashion_mnist\",\n",
    "    train=True,\n",
    "    download=False,\n",
    "    transform=torchvision.transforms.Compose(\n",
    "        [\n",
    "            torchvision.transforms.ToTensor(),\n",
    "            torchvision.transforms.Normalize((0.1307,), (0.3081,)),\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "\n",
    "fm_test_dataset = torchvision.datasets.FashionMNIST(\n",
    "    \"./fashion_mnist\",\n",
    "    train=False,\n",
    "    download=False,\n",
    "    transform=torchvision.transforms.Compose(\n",
    "        [\n",
    "            torchvision.transforms.ToTensor(),\n",
    "            torchvision.transforms.Normalize((0.1307,), (0.3081,)),\n",
    "        ]\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebffe0a",
   "metadata": {},
   "source": [
    "Next we apply the denoising model we trained on the MNIST data to FashionMNIST, and visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7ff2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(8):\n",
    "    visualize_denoising(unet_model, fm_train_dataset, 123 * i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1008f22c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><h4>\n",
    "    Task 5.2: </h4>\n",
    "What happened when the MNIST denoising model was applied to the FashionMNIST data? Why do you think the results look as they do?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e061b813",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><h4>\n",
    "    Task 5.3: </h4>\n",
    "Can you imagine any real-world scenarios where a denoising model would change the content of an image?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea0579f",
   "metadata": {},
   "source": [
    "### Train the denoiser on both MNIST and FashionMNIST\n",
    "\n",
    "In this section, we will perform the denoiser training once again, but this time on both MNIST and FashionMNIST datasets, and then try to apply the newly trained denoiser to a set of noisy test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cfb79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch\n",
    "\n",
    "# Some hyper-parameters:\n",
    "n_epochs = 3\n",
    "batch_size_train = 64\n",
    "batch_size_test = 1000\n",
    "\n",
    "# Dictionary to store loss history:\n",
    "history = {\"loss\": []}\n",
    "\n",
    "# Model:\n",
    "unet_model = UNet(depth=3, in_channels=1, upsample_mode=\"bilinear\")\n",
    "unet_model = unet_model.to(device)\n",
    "\n",
    "# Loss function:\n",
    "criterion = F.mse_loss  # mse_loss\n",
    "\n",
    "# Optimiser:\n",
    "optimizer = optim.Adam(unet_model.parameters(), lr=0.0005)\n",
    "\n",
    "# Train loader:\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    torch.utils.data.ConcatDataset([train_dataset, fm_train_dataset]),\n",
    "    batch_size=batch_size_train,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "# Training loop:\n",
    "for epoch in range(n_epochs):\n",
    "    train_denoising_model(train_loader, unet_model, criterion, optimizer, history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a88f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(8):\n",
    "    visualize_denoising(unet_model, test_dataset, 123 * i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35654265",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(8):\n",
    "    visualize_denoising(unet_model, fm_train_dataset, 123 * i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69980fc",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><h4>\n",
    "    Task 5.4: </h4>\n",
    "How does the new denoiser perform compared to the one from the previous section? Why?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0fce363",
   "metadata": {},
   "source": [
    "### Train the denoiser on both MNIST and FashionMNIST, shuffling the training data\n",
    "\n",
    "We previously performed the training sequentially on the MNIST data first then followed by the FashionMNIST data. Now, we ask for the training data to be shuffled and observe the impact on performance. (note the `shuffle=True` in the lines below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0f4a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch\n",
    "\n",
    "# Some hyper-parameters:\n",
    "n_epochs = 3\n",
    "batch_size_train = 64\n",
    "batch_size_test = 1000\n",
    "\n",
    "# Dictionary to store loss history:\n",
    "history = {\"loss\": []}\n",
    "\n",
    "# Model:\n",
    "unet_model = UNet(depth=3, in_channels=1, upsample_mode=\"bilinear\")\n",
    "unet_model = unet_model.to(device)\n",
    "\n",
    "# Loss function:\n",
    "criterion = F.mse_loss  # mse_loss\n",
    "\n",
    "# Optimiser:\n",
    "optimizer = optim.Adam(unet_model.parameters(), lr=0.0005)\n",
    "\n",
    "# Train loader:\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    torch.utils.data.ConcatDataset([train_dataset, fm_train_dataset]),\n",
    "    batch_size=batch_size_train,\n",
    "    shuffle=True,\n",
    ")  # here we set shuffle = True\n",
    "\n",
    "# Training loop:\n",
    "for epoch in range(n_epochs):\n",
    "    train_denoising_model(train_loader, unet_model, criterion, optimizer, history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b40d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(8):\n",
    "    visualize_denoising(unet_model, test_dataset, 123 * i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0a23a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(8):\n",
    "    visualize_denoising(unet_model, fm_train_dataset, 123 * i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98de1d28",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><h4>\n",
    "    Task 5.5: </h4>\n",
    "How does the denoiser trained on shuffled data perform compared to the one trained sequentially on one dataset and then on the other?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5247de",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-block alert-success\"><h3>\n",
    "    Checkpoint 5</h3>\n",
    "    <ol>\n",
    "        Congrats on reaching the final checkpoint! Let us know and we'll discuss the questions once reaching critical mass.\n",
    "    </ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffa3ac9",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-block alert-warning\"><h3>\n",
    "    Bonus Questions</h3>\n",
    "    <ol>\n",
    "        <li>Go back to Part 4 and try another attribution method, such as <a href=\"https://captum.ai/api/saliency.html\">Saliency</a>, and see how the results differ.</li>\n",
    "    </ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f91815",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "all",
   "custom_cell_magics": "kql"
  },
  "kernelspec": {
   "display_name": "Python [conda env:06-failure-modes]",
   "language": "python",
   "name": "conda-env-06-failure-modes-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
