{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 7: Failure Mode And Limits of Deep Learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "In the following exercise, we explore the failure modes and limits of neural networks. \n",
    "Neural networks are powerful, but it is important to understand their limits and the predictable reasons that they fail. \n",
    "These exercises illustrate how the content of datasets, especially differences between the training and inference/test datasets, can affect the network's output in unexpected ways.\n",
    "<br></br>\n",
    "While neural networks are generally less interpretable than other types of machine learning, it is still important to investigate the \"internal reasoning\" of the network as much as possible to discover failure modes, or situations in which the network does not perform well. \n",
    "This exercise introduces a tool called Integrated Gradients that helps us makes sense of the network \"attention\". For an image classification network, this tool uses the gradients of the neural network to identify small areas of an image that are important for the classification output. \n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Overview:\n",
    "In this exercise you will...\n",
    "1. Tamper with a dataset and introduce additional visual information for some classes. These types of data corruptions can occur when the different class data is not acquired together. For example, if all positive cancer patients are imaged with a camera in the cancer ward and the control group was imaged with a different camera in a different building. Let's explore what this means for our machine learning model.\n",
    "\n",
    "2. Explore the inner workings of an image classification network using `IntegratedGradients`.\n",
    "\n",
    "3. Explore how networks deal with or struggle with domain changes.\n",
    "\n",
    "<div class=\"alert alert-danger\">\n",
    "Set your python kernel to <code>07-failure-modes</code>\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acknowledgements\n",
    "This notebook was created by Steffen Wolf, Jordao Bragantini, Jan Funke, and Loic Royer. Modified by Tri Nguyen, Igor Zubarev, and Morgan Schwartz for DL@MBL 2022, and Caroline Malin-Mayor for DL@MBL 2023."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading\n",
    "\n",
    "The following will load the MNIST dataset, which already comes split into a training and testing dataset.\n",
    "The MNIST dataset contains images of handwritten digits 0-9.\n",
    "This data was already downloaded in the setup script.\n",
    "Documentation for this pytorch dataset is available at https://pytorch.org/vision/main/generated/torchvision.datasets.MNIST.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST('./mnist', train=True, download=False,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ]))\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST('./mnist', train=False, download=False,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Preparation of a Tainted Dataset\n",
    "\n",
    "In this section we will make small changes to specific classes that will let the models 'cheat'.\n",
    "We expect that the models will focus their atttention on these hints instead of the digits themselves.\n",
    "For example, if we add a white pixel in the bottom right of all images of 7s, the network can use that pixel to determine if the image is a 7, rather than looking at the actual digit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports:\n",
    "import torch\n",
    "import numpy\n",
    "from scipy.ndimage import convolve\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create copies so we do not modify the original datasets:\n",
    "tainted_train_dataset = copy.deepcopy(train_dataset)\n",
    "tainted_test_dataset = copy.deepcopy(test_dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><h4>\n",
    "Task 1.1: Add White Pixel to 7s</h4>\n",
    "\n",
    "Add a white pixel in the bottom right of all images of 7's. \n",
    "Starter code is provided below. \n",
    "`train_dataset.targets==7, 25, 25` is used to index into the data, selecting the pixel at (25, 25) of all samples that are labeled seven. \n",
    "First fill in the value of a white pixel to taint the training data (it may be helpful to figure out what type the data is by printing `tainted_train_dataset.data.dtype`). Then repeat for the tainted test data.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taint the dataset with a white pixel in the bottom right of all images of 7's\n",
    "tainted_train_dataset.data[train_dataset.targets==7, 25, 25] = #...  # replace the ... with a value that will make the pixel white \n",
    "# Then add another line to taint the test dataset as well\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in the next cell will visualize a few examples of 1s and 7s from the training and test datasets. If you performed task 1.1 correctly, you should see a white pixel in the lower right corner of the 7s and not the 1s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.subplot(1,4,1)\n",
    "plt.imshow(tainted_train_dataset[3][0][0], cmap=plt.get_cmap('gray'))\n",
    "plt.subplot(1,4,2)\n",
    "plt.imshow(tainted_train_dataset[23][0][0], cmap=plt.get_cmap('gray'))\n",
    "plt.subplot(1,4,3)\n",
    "plt.imshow(tainted_train_dataset[15][0][0], cmap=plt.get_cmap('gray'))\n",
    "plt.subplot(1,4,4)\n",
    "plt.imshow(tainted_train_dataset[29][0][0], cmap=plt.get_cmap('gray'))\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some data corruption or domain differences cover the whole image, rather than being localized to a specific location. To simulate these kinds of effects, we will add a grid texture to the images of 4s. \n",
    "\n",
    "You may have noticed in Task 1.1 that the images are stores as integers. First we cast them to float to be able to add textures easily without integer wrapping issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast to float\n",
    "tainted_train_dataset.data = tainted_train_dataset.data.type(torch.FloatTensor) \n",
    "tainted_test_dataset.data = tainted_test_dataset.data.type(torch.FloatTensor)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we create the grid texture and visualize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create grid texture\n",
    "texture = numpy.zeros(tainted_test_dataset.data.shape[1:])\n",
    "texture[::2,::2] = 80 \n",
    "texture = convolve(texture, weights=[[0.5,1,0.5],[1,0.1,0.5],[1,0.5,0]])\n",
    "texture = torch.from_numpy(texture)\n",
    "\n",
    "plt.imshow(texture, cmap=plt.get_cmap('gray'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><h4>\n",
    "Task 1.2: Add Grid Texture to 4s</h4>\n",
    "\n",
    "Add the grid texture to all images of 4s in the tainted train and test datasets.\n",
    "This will be similar to Task 1.1, but instead of selecting the pixel at (25, 25),\n",
    "we want to simply select the whole image using the index `train_dataset.targets==4`, and then add the texture to the existing value, rather than replacing it with a white pixel.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1.2: Add texture to all images of 4s here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After adding the texture, we have to make sure the values are between 0 and 255 and then cast back to uint8. \n",
    "Then we visualize a couple 4s from the dataset to see if the grid texture has been added properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clamp all images to avoid values above 255 that might occur:\n",
    "tainted_train_dataset.data = torch.clamp(tainted_train_dataset.data, 0, 255)\n",
    "tainted_test_dataset.data  = torch.clamp(tainted_test_dataset.data, 0, 255)\n",
    "\n",
    "# Cast back to byte:\n",
    "tainted_train_dataset.data = tainted_train_dataset.data.type(torch.uint8) \n",
    "tainted_test_dataset.data = tainted_test_dataset.data.type(torch.uint8) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize example 4s\n",
    "plt.subplot(1,4,1)\n",
    "plt.imshow(tainted_train_dataset[9][0][0], cmap=plt.get_cmap('gray'))\n",
    "plt.subplot(1,4,2)\n",
    "plt.imshow(tainted_train_dataset[26][0][0], cmap=plt.get_cmap('gray'))\n",
    "plt.subplot(1,4,3)\n",
    "plt.imshow(tainted_train_dataset[20][0][0], cmap=plt.get_cmap('gray'))\n",
    "plt.subplot(1,4,4)\n",
    "plt.imshow(tainted_train_dataset[53][0][0], cmap=plt.get_cmap('gray'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"><h3>\n",
    "    Questions:</h3>\n",
    "    <ol>\n",
    "        <li>Why are we tainting the datasets in this exercise? How can real datasets be tainted in similar manners?</li>\n",
    "        <li>How do you think the network will perform when trained on *tainted* data and tested on *tainted* data?</li>\n",
    "        <li>How do you think the network will perform when trained on *tainted* data and tested on *untainted* data?</li>\n",
    "    </ol>\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Create and Train an Image Classification Neural Network on Clean and Tainted Data\n",
    "\n",
    "From Part 1, we have a clean dataset and a dataset that has been tainted with effects that simulate local and global effects that could happen in real collection scenarios. Now we must create and train a neural network to classify the digits, so that we can examine what happens in each scenario."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><h4>\n",
    "Task 2.1 (Repeat): Implement a Densely Connected Neural Network </h4>\n",
    "\n",
    "Boilerplate code to create the neural network is provided below. The network class is named `DenseModel`.and inherits from `torch.nn.Module`.\n",
    "\n",
    "First, initialize the linear layers inside the `__init__()` function. You should create 4 fully connected layers with sizes [784, 256, 120, 84, 10] (note: 784 is the input size).\n",
    "\n",
    "Next, fill in the `forward()` function as follows:\n",
    "* First, flatten the input image into a 1d tensor (2d counting the batch dimension) using `torch.flatten`.\n",
    "* Then, apply each linear layer that you initialized previously, following each with relu activation except the final layer. \n",
    "* Return the output.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "    \n",
    "class DenseModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # initialize linear layers here\n",
    "        self.fc0 = ...\n",
    "\n",
    "    def forward(self, x):\n",
    "        # apply all layers and return the final output\n",
    "        pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will train the neural network. A training function is provided below - this should be familiar, but make sure you look it over and understand what is happening in the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Training function:\n",
    "def train_mnist(model, train_loader, batch_size, criterion, optimizer, history):\n",
    "    model.train()\n",
    "    pbar = tqdm(total=len(tainted_train_dataset)//batch_size)\n",
    "    for batch_idx, (raw, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        raw = raw.cuda()\n",
    "        target = target.cuda()\n",
    "        output = model(raw)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        history.append(loss.item())\n",
    "        pbar.update(1)\n",
    "    return history"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to choose hyperparameters for our model. We have selected to train for two epochs, with a batch size of 64 for training and 1000 for testing. We are using the cross entropy loss, a standard multi-class classification loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch\n",
    "\n",
    "# Let's set some hyperparameters:\n",
    "n_epochs = 2\n",
    "batch_size_train = 64\n",
    "batch_size_test = 1000\n",
    "\n",
    "# Loss function:\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we initialize a clean model, and a tainted model. We want to have reproducible results, so we set the initial weights with a specific random seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the clean and tained models\n",
    "model_clean = DenseModel().cuda()\n",
    "model_tainted = DenseModel().cuda()\n",
    "\n",
    "# Weight initialisation:\n",
    "def init_weights(m):\n",
    "    if isinstance(m, (nn.Linear, nn.Conv2d)):\n",
    "        torch.nn.init.xavier_uniform_(m.weight, )\n",
    "        m.bias.data.fill_(0.01)\n",
    "   \n",
    "# Fixing seed with magical number and setting weights:\n",
    "torch.random.manual_seed(42)\n",
    "model_clean.apply(init_weights)\n",
    "\n",
    "# Fixing seed with magical number and setting weights:\n",
    "torch.random.manual_seed(42)\n",
    "model_tainted.apply(init_weights)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we initialize the clean and tainted dataloaders, again with a specific random seed for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising dataloaders:\n",
    "train_loader_tainted = torch.utils.data.DataLoader(tainted_train_dataset,\n",
    "  batch_size=batch_size_train, shuffle=True, generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "  batch_size=batch_size_train, shuffle=True, generator=torch.Generator().manual_seed(42))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><h4>\n",
    "    Task 2.2: Train the neural networks!</h4>\n",
    "\n",
    "\n",
    "Now you will put together all the setup above to actually train the models.\n",
    "For each model (clean and tainted), create a for loop that will loop for the number of epochs specified above. Inside the loop, call the train function with the following arguments:\n",
    "* model: the clean or tainted model that we initialized above \n",
    "* dataloader: the clean or tainted dataloader we initialized above\n",
    "* batch size: the training batch size we defined in the hyperparameters section\n",
    "* criterion: the loss function we defined in the hyperparameters section\n",
    "* optimiser: an Adam optimizer with the clean or tainted parameters and learning rate 0.001. For example `optim.Adam(model_clean.parameters(), lr=0.001)`.\n",
    "* history: The location to store the clean/tainted history. We create a dictionary for this purpose so that the history can be appended across epochs. Then, we pass in the dictionary value, for example, `history['loss_tainted']`, and the train function will update the history inside the dictionary.\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We store history here:\n",
    "history = {\"loss_tainted\": [],\n",
    "           \"loss_clean\": []}\n",
    "\n",
    "# Training loop for clean model:\n",
    "\n",
    "# Training loop for tainted model:\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we visualize the loss history for the clean and tainted models. If you implemented Task 2.1 and 2.2 properly, the loss should decrease from around 2.5 to less than 0.5 for both models, and the tainted model should have a slightly lower training loss than the clean model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the loss history:\n",
    "fig = plt.figure()\n",
    "plt.plot(history[\"loss_clean\"], color='blue')\n",
    "plt.plot(history[\"loss_tainted\"], color='red')\n",
    "plt.legend(['Train Loss Clean', \"Train Loss Tainted\"], loc='upper right')\n",
    "plt.xlabel('number of training examples seen')\n",
    "plt.ylabel('negative log likelihood loss')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"><h3>\n",
    "    Questions:</h3>\n",
    "    <ol>\n",
    "        <li>Why do you think the tainted network has lower training loss than the clean network?</li>\n",
    "        <li>Do you think the tainted network will be more accurate than the clean network when applied to the tainted test data? Why?</li>\n",
    "        <li>Do you think the tainted network will be more accurate than the clean network when applied to the clean test data? Why?</li>\n",
    "    </ol>\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\"><h3>\n",
    "    Checkpoint 1</h3>\n",
    "\n",
    "Post to Element when you have reached Checkpoint 1. We will discuss our predictions!\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Examining the Results of the Clean and Tainted Networks\n",
    "\n",
    "Now that we have initialized our clean and tainted datasets and trained our models on them, it is time to examine how these models perform on the clean and tainted test sets!\n",
    "\n",
    "We provide a `predict` function below that will return the prediction and ground truth labels given a particualr model and dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# predict the test dataset\n",
    "def predict(model, dataset):\n",
    "    dataset_prediction = []\n",
    "    dataset_groundtruth = []\n",
    "    with torch.no_grad():\n",
    "        for x, y_true in dataset:\n",
    "            inp = x[None].cuda()\n",
    "            y_pred = model(inp)\n",
    "            dataset_prediction.append(y_pred.argmax().cpu().numpy())\n",
    "            dataset_groundtruth.append(y_true)\n",
    "    \n",
    "    return np.array(dataset_prediction), np.array(dataset_groundtruth)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><h4>\n",
    "    Task 3.1: Make Predictions on Test Sets</h4>\n",
    "\n",
    "We will want to examine how each model performs on each test set. Therefore, you should make four sets of predictions: clean model on clean test, clean model on tainted test, tainted model on clean test, and tainted model on tainted test. \n",
    "\n",
    "Recall that the test datasets we defined back in part 1 are called `test_dataset` and `tainted_test_dataset`.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the predict function using different models and test datasets\n",
    "# Note: All the true labels are the same, since we only changed the data and not the labels,\n",
    "# so we just save the values once as true_labels\n",
    "\n",
    "pred_clean_clean, true_labels = ...\n",
    "pred_clean_tainted, _ = ...\n",
    "pred_tainted_clean, _ = ...\n",
    "pred_tainted_tainted, _ = ... "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can investivate the results using the confusion matrix. The function in the cell below will create a nicely annotated confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "# Plot confusion matrix \n",
    "# orginally from Runqi Yang; \n",
    "# see https://gist.github.com/hitvoice/36cf44689065ca9b927431546381a3f7\n",
    "def cm_analysis(y_true, y_pred, title, figsize=(10,10)):\n",
    "    \"\"\"\n",
    "    Generate matrix plot of confusion matrix with pretty annotations.\n",
    "    The plot image is saved to disk.\n",
    "    args: \n",
    "      y_true:    true label of the data, with shape (nsamples,)\n",
    "      y_pred:    prediction of the data, with shape (nsamples,)\n",
    "      filename:  filename of figure file to save\n",
    "      labels:    string array, name the order of class labels in the confusion matrix.\n",
    "                 use `clf.classes_` if using scikit-learn models.\n",
    "                 with shape (nclass,).\n",
    "      ymap:      dict: any -> string, length == nclass.\n",
    "                 if not None, map the labels & ys to more understandable strings.\n",
    "                 Caution: original y_true, y_pred and labels must align.\n",
    "      figsize:   the size of the figure plotted.\n",
    "    \"\"\"\n",
    "    labels = [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"]\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    cm_sum = np.sum(cm, axis=1, keepdims=True)\n",
    "    cm_perc = cm / cm_sum.astype(float) * 100\n",
    "    annot = np.empty_like(cm).astype(str)\n",
    "    nrows, ncols = cm.shape\n",
    "    for i in range(nrows):\n",
    "        for j in range(ncols):\n",
    "            c = cm[i, j]\n",
    "            p = cm_perc[i, j]\n",
    "            if i == j:\n",
    "                s = cm_sum[i]\n",
    "                annot[i, j] = '%.1f%%\\n%d/%d' % (p, c, s)\n",
    "            elif c == 0:\n",
    "                annot[i, j] = ''\n",
    "            else:\n",
    "                annot[i, j] = '%.1f%%\\n%d' % (p, c)\n",
    "    cm = pd.DataFrame(cm, index=labels, columns=labels)\n",
    "    cm.index.name = 'Actual'\n",
    "    cm.columns.name = 'Predicted'\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    ax=sns.heatmap(cm, annot=annot, fmt='', vmax=30)\n",
    "    ax.set_title(title)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will generate confusion matrices for each model/data combination. Take your time and try and interpret these, and then try and answer the questions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_analysis(true_labels, pred_clean_clean, \"Clean Model on Clean Data\")\n",
    "cm_analysis(true_labels, pred_clean_tainted, \"Clean Model on Tainted Data\")\n",
    "cm_analysis(true_labels, pred_tainted_clean, \"Tainted Model on Clean Data\")\n",
    "cm_analysis(true_labels, pred_tainted_tainted, \"Tainted Model on Tainted Data\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"><h3>\n",
    "    Questions:</h3>\n",
    "    <ol>\n",
    "        <li>Which model/data combination had the best overall performance? Worst?</li>\n",
    "        <li>Which digits were most confused with each other? Did this change between tainted/clean models/datasets?</li>\n",
    "        <li>Why do you think the differences noted above happened?</li>\n",
    "    </ol>\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4: Interpretation with Integrated Gradients\n",
    "Perhaps you formed some hypotheses about why the clean and tainted models did better or worse on certain datasets in the previous section. Now we will use an attribution algorithm called `IntegratedGradients` to learn more about the inner workings of each model. This algorithm attempts to use the gradients of the network to attribute the label of the image (digit class) to specific input pixels, and then visualize the results."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><h4>\n",
    "    Task 4.1: Read about Integrated Gradients and the Captum Library</h4>\n",
    "\n",
    "Familiarize yourself with the captum library by reading [this useful tutorial](https://captum.ai/tutorials/CIFAR_TorchVision_Interpret)\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><h4>\n",
    "    Task 4.2: Apply Integrated Gradients </h4>\n",
    "\n",
    "Now that you are familiar with the library, we will write a function to apply integrated gradients to a model.\n",
    "\n",
    "Below we have provided most of the  `apply_integrated_gradients` function. \n",
    "You will fill in the line to actually apply the algorithm to the input image using `IntegratedGradients.attribute()` (API documentation at https://captum.ai/api/integrated_gradients.html).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from captum.attr import IntegratedGradients\n",
    "\n",
    "def apply_integrated_gradients(test_input, model):\n",
    "    # move the model to cpu\n",
    "    model.cpu()\n",
    "    \n",
    "    # initialize algorithm\n",
    "    algorithm = IntegratedGradients(model)\n",
    "\n",
    "    # clear the gradients from the model\n",
    "    model.zero_grad()\n",
    "\n",
    "    # Get input and target tensors from test_input\n",
    "    input_tensor = test_input[0].unsqueeze(0)\n",
    "    input_tensor.requires_grad = True\n",
    "    target = test_input[1]\n",
    "\n",
    "    # Run attribution:\n",
    "    attributions = ### YOUR CODE HERE ###\n",
    "\n",
    "    return attributions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we provide a function to visualize the output of integrated gradients. It calls `apply_integrated_gradients` to use the code you wrote above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from captum.attr import visualization as viz\n",
    "\n",
    "def visualize_integrated_gradients(test_input, model):\n",
    "    attr_ig = apply_integrated_gradients(test_input, model)\n",
    "\n",
    "    # Transpose integrated gradients output\n",
    "    attr_ig = np.transpose(attr_ig[0].cpu().detach().numpy(), (1, 2, 0))\n",
    "    \n",
    "    # Transpose and normalize original image:\n",
    "    original_image = np.transpose((test_input[0].detach().numpy() * 0.5) + 0.5, (1, 2, 0))\n",
    "\n",
    "    # This visualises the attribution of labels to pixels\n",
    "    viz.visualize_image_attr(attr_ig, \n",
    "                             original_image, \n",
    "                             method=\"blended_heat_map\",\n",
    "                             sign=\"all\",\n",
    "                             show_colorbar=True, \n",
    "                             title=\"Overlayed Integrated Gradients\",\n",
    "                             fig_size=(3,3))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><h4>\n",
    "    Task 4.3: Visualize Integrated Gradients Examples</h4>\n",
    "\n",
    "Call the `visualize_integrated_gradients` with...\n",
    "\n",
    "* the \"tainted\" classifier on a \"tainted\" image of number 7\n",
    "* the \"clean\" classifier on a \"tainted\" image of number 7\n",
    "* the \"tainted\" classifier on a \"clean\" image of number 7\n",
    "* of the \"clean\" classifier on a \"clean\" image of number 7\n",
    "* of the \"tainted\" classifier on a \"tainted\" image of number 4\n",
    "* of the \"clean\" classifier on a \"tainted\" image of number 4\n",
    "\n",
    "You may need to search through the test dataset labels (`true_labels`) to find indices of 7s and 4s.\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"><h3>\n",
    "    Questions</h3>\n",
    "    <ol>\n",
    "        <li>Where did the \"tainted\" model focus its attention on for the tainted \"4\" and \"7\"?</li>\n",
    "        <li>Where did the \"clean\" model focus its attention on for the tainted \"4\" and \"7\"?</li>\n",
    "        <li>How do these results help you interpret the confusion matrices? Were your predictions correct about why certain models did better or worse on certain digits?</li>\n",
    "    </ol>\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"><h3>\n",
    "    Checkpoint 2</h3>\n",
    "    <ol>\n",
    "        Congrats on finishing the intergrated gradients task! Let us know on Element, and feel free to try other methods mentioned in the linked tutorial if you're interested.\n",
    "    </ol>\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Importance of using the right training data\n",
    "\n",
    "Now we will move on from image classification to denoising, and show why it is particularly important to ensure that your training and test data are from the same distribution for these kinds of networks.\n",
    "\n",
    "For this exercise, we will first train a simple CNN model to denoise MNIST images of digits, and then apply it to the Fashion MNIST to see what happens when the training and inference data are mismatched.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will write a function to add noise to the MNIST dataset, so that we can train a model to denoise it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# A simple function to add noise to tensors:\n",
    "def add_noise(tensor, power=1.5):\n",
    "    return tensor * torch.rand(tensor.size()).to(tensor.device) ** power + 0.75*torch.randn(tensor.size()).to(tensor.device)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will visualize a couple MNIST examples with and without noise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Let's visualise MNIST images with noise:\n",
    "def show(index):\n",
    "    plt.subplot(1,4,1)\n",
    "    plt.imshow(train_dataset[index][0][0], cmap=plt.get_cmap('gray'))\n",
    "    plt.subplot(1,4,2)\n",
    "    plt.imshow(add_noise(train_dataset[index][0][0]), cmap=plt.get_cmap('gray'))\n",
    "    plt.subplot(1,4,3)\n",
    "    plt.imshow(train_dataset[index+1][0][0], cmap=plt.get_cmap('gray'))\n",
    "    plt.subplot(1,4,4)\n",
    "    plt.imshow(add_noise(train_dataset[index+1][0][0]), cmap=plt.get_cmap('gray'))\n",
    "    plt.show()\n",
    "\n",
    "# We pick 8 images to show:\n",
    "for i in range(8):\n",
    "    show(123*i)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UNet model\n",
    "\n",
    "Let's try denoising with a UNet, \"CARE-style\". As UNets and denoising implementations are not the focus of this exercise, we provide the model for you in the following cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from https://discuss.pytorch.org/t/unet-implementation/426\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels=1,\n",
    "        n_classes=1,\n",
    "        depth=3,\n",
    "        wf=4,\n",
    "        padding=True,\n",
    "        batch_norm=False,\n",
    "        up_mode='upsample',\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Implementation of\n",
    "        U-Net: Convolutional Networks for Biomedical Image Segmentation\n",
    "        (Ronneberger et al., 2015)\n",
    "        https://arxiv.org/abs/1505.04597\n",
    "        Using the default arguments will yield the exact version used\n",
    "        in the original paper\n",
    "        Args:\n",
    "            in_channels (int): number of input channels\n",
    "            n_classes (int): number of output channels\n",
    "            depth (int): depth of the network\n",
    "            wf (int): number of filters in the first layer is 2**wf\n",
    "            padding (bool): if True, apply padding such that the input shape\n",
    "                            is the same as the output.\n",
    "                            This may introduce artifacts\n",
    "            batch_norm (bool): Use BatchNorm after layers with an\n",
    "                               activation function\n",
    "            up_mode (str): one of 'upconv' or 'upsample'.\n",
    "                           'upconv' will use transposed convolutions for\n",
    "                           learned upsampling.\n",
    "                           'upsample' will use bilinear upsampling.\n",
    "        \"\"\"\n",
    "        super(UNet, self).__init__()\n",
    "        assert up_mode in ('upconv', 'upsample')\n",
    "        self.padding = padding\n",
    "        self.depth = depth\n",
    "        prev_channels = in_channels\n",
    "        self.down_path = nn.ModuleList()\n",
    "        for i in range(depth):\n",
    "            self.down_path.append(\n",
    "                UNetConvBlock(prev_channels, 2 ** (wf + i), padding, batch_norm)\n",
    "            )\n",
    "            prev_channels = 2 ** (wf + i)\n",
    "\n",
    "        self.up_path = nn.ModuleList()\n",
    "        for i in reversed(range(depth - 1)):\n",
    "            self.up_path.append(\n",
    "                UNetUpBlock(prev_channels, 2 ** (wf + i), up_mode, padding, batch_norm)\n",
    "            )\n",
    "            prev_channels = 2 ** (wf + i)\n",
    "\n",
    "        self.last = nn.Conv2d(prev_channels, n_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        blocks = []\n",
    "        for i, down in enumerate(self.down_path):\n",
    "            x = down(x)\n",
    "            if i != len(self.down_path) - 1:\n",
    "                blocks.append(x)\n",
    "                x = F.max_pool2d(x, 2)\n",
    "\n",
    "        for i, up in enumerate(self.up_path):\n",
    "            x = up(x, blocks[-i - 1])\n",
    "\n",
    "        return self.last(x)\n",
    "\n",
    "\n",
    "class UNetConvBlock(nn.Module):\n",
    "    def __init__(self, in_size, out_size, padding, batch_norm):\n",
    "        super(UNetConvBlock, self).__init__()\n",
    "        block = []\n",
    "\n",
    "        block.append(nn.Conv2d(in_size, out_size, kernel_size=3, padding=int(padding)))\n",
    "        block.append(nn.ReLU())\n",
    "        if batch_norm:\n",
    "            block.append(nn.BatchNorm2d(out_size))\n",
    "\n",
    "        block.append(nn.Conv2d(out_size, out_size, kernel_size=3, padding=int(padding)))\n",
    "        block.append(nn.ReLU())\n",
    "        if batch_norm:\n",
    "            block.append(nn.BatchNorm2d(out_size))\n",
    "\n",
    "        self.block = nn.Sequential(*block)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.block(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "class UNetUpBlock(nn.Module):\n",
    "    def __init__(self, in_size, out_size, up_mode, padding, batch_norm):\n",
    "        super(UNetUpBlock, self).__init__()\n",
    "        if up_mode == 'upconv':\n",
    "            self.up = nn.ConvTranspose2d(in_size, out_size, kernel_size=2, stride=2)\n",
    "        elif up_mode == 'upsample':\n",
    "            self.up = nn.Sequential(\n",
    "                nn.Upsample(mode='bilinear', scale_factor=2),\n",
    "                nn.Conv2d(in_size, out_size, kernel_size=1),\n",
    "            )\n",
    "\n",
    "        self.conv_block = UNetConvBlock(in_size, out_size, padding, batch_norm)\n",
    "\n",
    "    def center_crop(self, layer, target_size):\n",
    "        _, _, layer_height, layer_width = layer.size()\n",
    "        diff_y = (layer_height - target_size[0]) // 2\n",
    "        diff_x = (layer_width - target_size[1]) // 2\n",
    "        return layer[\n",
    "            :, :, diff_y : (diff_y + target_size[0]), diff_x : (diff_x + target_size[1])\n",
    "        ]\n",
    "\n",
    "    def forward(self, x, bridge):\n",
    "        up = self.up(x)\n",
    "        crop1 = self.center_crop(bridge, up.shape[2:])\n",
    "        out = torch.cat([up, crop1], 1)\n",
    "        out = self.conv_block(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training loop code is also provided here. It is similar to the code used to train the image classification model previously, but look it over to make sure there are no surprises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def train_denoising_model(dataset, model, criterion, optimizer, history):\n",
    "    \n",
    "    # Puts model in 'training' mode:\n",
    "    model.train()\n",
    "    \n",
    "    # Initialises progress bar:\n",
    "    pbar = tqdm(total=len(dataset)//batch_size_train)\n",
    "    for batch_idx, (image, target) in enumerate(train_loader):\n",
    "\n",
    "        # add line here during Task 2.2\n",
    "        \n",
    "        # Zeroing gradients:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Moves image to GPU memory:\n",
    "        image = image.cuda()\n",
    "        \n",
    "        # Adds noise to make the noisy image:\n",
    "        noisy = add_noise(image)\n",
    "        \n",
    "        # Runs model on noisy image:\n",
    "        output = model(noisy)\n",
    "        \n",
    "        # Computes loss:\n",
    "        loss = criterion(output, image)\n",
    "        \n",
    "        # Backpropagates gradients:\n",
    "        loss.backward()\n",
    "        \n",
    "        # Optimises model parameters given the current gradients:\n",
    "        optimizer.step()\n",
    "        \n",
    "        # appends loss history:\n",
    "        history[\"loss\"].append(loss.item())\n",
    "        \n",
    "        # updates progress bar:\n",
    "        pbar.update(1)\n",
    "    return history"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we choose hyperparameters and initialize the model and data loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch\n",
    "\n",
    "# Some hyper-parameters:\n",
    "n_epochs = 5\n",
    "batch_size_train = 64\n",
    "batch_size_test = 1000\n",
    "\n",
    "# Dictionary to store loss history:\n",
    "history = {\"loss\": []}\n",
    "\n",
    "# Model:\n",
    "unet_model = UNet().cuda()\n",
    "\n",
    "# Loss function:\n",
    "criterion = F.mse_loss #mse_loss\n",
    "\n",
    "# Optimiser:\n",
    "optimizer = optim.Adam(unet_model.parameters(), lr=0.0005)\n",
    "\n",
    "# Test loader:\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset,\n",
    "  batch_size=batch_size_test, shuffle=True)\n",
    "\n",
    "# Train loader:\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "  batch_size=batch_size_train, shuffle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we run the training loop! This may take a little while - feel free to look more into attribution methods or chat with your neighbor about what will happen when we apply this model to the Fashion MNIST data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop:\n",
    "for epoch in range(n_epochs):\n",
    "    train_denoising_model(train_dataset, unet_model, criterion, optimizer, history)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we will visualize the training loss. If all went correctly, it should decrease from around 1.0 to less than 0.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the loss over time:\n",
    "fig = plt.figure()\n",
    "plt.plot(history[\"loss\"], color='blue')\n",
    "plt.legend(['Train Loss'], loc='upper right')\n",
    "plt.xlabel('number of training examples seen')\n",
    "plt.ylabel('mean squared error loss')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check denoising performance\n",
    "\n",
    "We see that the training loss decreased, but let's apply the model to the test set to see how well it was able to recover the digits from the noisy images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_denoising(image, model):\n",
    "    # add batch and channel dimensions\n",
    "    image = torch.unsqueeze(torch.unsqueeze(image, 0), 0)\n",
    "    prediction = model(image)\n",
    "    # remove batch and channel dimensions before returning\n",
    "    return prediction.detach().cpu()[0,0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displays: ground truth, noisy, and denoised images\n",
    "def visualize_denoising(model, dataset, index):\n",
    "    orig_image = dataset[index][0][0]\n",
    "    noisy_image = add_noise(orig_image)\n",
    "    denoised_image = apply_denoising(noisy_image, model)\n",
    "    plt.subplot(1,4,1)\n",
    "    plt.imshow(orig_image, cmap=plt.get_cmap('gray'))\n",
    "    plt.subplot(1,4,2)\n",
    "    plt.imshow(noisy_image, cmap=plt.get_cmap('gray'))\n",
    "    plt.subplot(1,4,3)\n",
    "    plt.imshow(denoised_image, cmap=plt.get_cmap('gray'))\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# We pick 8 images to show:\n",
    "for i in range(8):\n",
    "    visualize_denoising(unet_model, test_dataset, 123*i)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"><h3>\n",
    "    Questions</h3>\n",
    "    <ol>\n",
    "        <li>Did the denoising net trained on MNIST work well on unseen test data?</li>\n",
    "        <li>Do you think it would work well with other unseen data?</li>\n",
    "    </ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply trained model on 'wrong' data \n",
    "\n",
    "Apply the denoising model trained above to some example _noisy_ images derived from the Fashion-MNIST dataset.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Fashion MNIST dataset\n",
    "\n",
    "Similar to the regular MNIST, we will use the pytorch FashionMNIST dataset. This was downloaded in the setup.sh script, so here we are just loading it into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm_train_dataset = torchvision.datasets.FashionMNIST('./fashion_mnist', train=True, download=False,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ]))\n",
    "\n",
    "fm_test_dataset = torchvision.datasets.FashionMNIST('./fashion_mnist', train=False, download=False,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-info\"><h4>\n",
    "    Task 5.1: Apply the Denoising Network to Fashion MNIST</h4>\n",
    "\n",
    "Use the `apply_denoising` and/or `visualize_denoising` functions defined above to apply our denoising `unet_model` to the fashion mnist data. Visualize some results and take your time to interpret them.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Implement your solution here.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"><h3>\n",
    "    Questions</h3>\n",
    "    <ol>\n",
    "        <li>Did the denoising net trained on MNIST work well on Fashion MNIST?</li>\n",
    "        <li>Why or why not do you think?</li>\n",
    "        <li>What kind of scenarios might this pitfall happen in the real world?</li>\n",
    "        <li>Any idea on how to make the network more generalized?</li>\n",
    "    </ol>\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"><h3>\n",
    "    Checkpoint 3</h3>\n",
    "    <ol>\n",
    "        Congrats on reaching the final checkpoint! Let us know on Element, and we'll discuss the questions once reaching critical mass.\n",
    "    </ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"><h3>\n",
    "    Extra credits</h3>\n",
    "    <ol>\n",
    "        <li>Decimate training data: In the code above, add filter every training batch so that it only contains images of class \"1\". Skip the batch if there are no \"1\"s in the batch.</li>\n",
    "        <li>Redo the exercises but swap MNIST with Fashion-MNIST</li>\n",
    "        <li>Don't hesitate to try things, explore the behaviour of the network... For example, you can also try reducing the size of the training data, can you cause the network to overfit? How would you cause the network to underfit?</li>\n",
    "    </ol>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:07_failure_modes]",
   "language": "python",
   "name": "conda-env-07_failure_modes-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
