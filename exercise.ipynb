{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 7: Failure Modes And Limits of Deep Learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following exercise, we explore the failure modes and limits of neural networks. \n",
    "Neural networks are powerful, but it is important to understand their limits and the predictable reasons that they fail. \n",
    "These exercises illustrate how the content of datasets, especially differences between the training and inference/test datasets, can affect the network's output in unexpected ways.\n",
    "<br></br>\n",
    "While neural networks are generally less interpretable than other types of machine learning, it is still important to investigate the \"internal reasoning\" of the network as much as possible to discover failure modes, or situations in which the network does not perform well. \n",
    "This exercise introduces a tool called Integrated Gradients that helps us makes sense of the network \"attention\". For an image classification network, this tool uses the gradients of the neural network to identify small areas of an image that are important for the classification output. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Overview:\n",
    "In this exercise you will...\n",
    "1. Tamper with an image dataset and introduce additional visual information for some classes. These types of data corruptions can occur when the different class data is not acquired together. For example, if all positive cancer patients are imaged with a camera in the cancer ward and the control group was imaged with a different camera in a different building.\n",
    "\n",
    "2. Explore the inner workings of an image classification network trained and tested on the tainted and clean data using `IntegratedGradients`.\n",
    "\n",
    "3. Explore how denoising networks deal with or struggle with domain changes.\n",
    "\n",
    "*NOTE*: There is very little coding in this exercise, as the goal is for you to think deeply about how neural networks can be influenced by differences in data. We encourage you to think deeply about the questions and discuss them in small groups, as well as with the full class during the frequent checkpoints.\n",
    "\n",
    "<div class=\"alert alert-danger\">\n",
    "Set your python kernel to <code>07-failure-modes</code>\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acknowledgements\n",
    "This notebook was created by Steffen Wolf, Jordao Bragantini, Jan Funke, and Loic Royer. Modified by Tri Nguyen, Igor Zubarev, and Morgan Schwartz for DL@MBL 2022, and Caroline Malin-Mayor for DL@MBL 2023."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading\n",
    "\n",
    "The following will load the MNIST dataset, which already comes split into a training and testing dataset.\n",
    "The MNIST dataset contains images of handwritten digits 0-9.\n",
    "This data was already downloaded in the setup script.\n",
    "Documentation for this pytorch dataset is available at https://pytorch.org/vision/main/generated/torchvision.datasets.MNIST.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST('./mnist', train=True, download=False,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ]))\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST('./mnist', train=False, download=False,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Preparation of a Tainted Dataset\n",
    "\n",
    "In this section we will make small changes to specific classes of data in the MNIST dataset. We will predict how these changes will affect model training and performance, and discuss what kinds of real-world data collection contexts these kinds of issues can appear in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports:\n",
    "import torch\n",
    "import numpy\n",
    "from scipy.ndimage import convolve\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create copies so we do not modify the original datasets:\n",
    "tainted_train_dataset = copy.deepcopy(train_dataset)\n",
    "tainted_test_dataset = copy.deepcopy(test_dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1.1: Local Corruption of Data\n",
    "\n",
    "First we will add a white pixel in the bottom right of all images of 7's, and visualize the results. This is an example of a local change to the images, where only a small portion of the image is corruped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Add a white pixel in the bottom right of all images of 7's\n",
    "tainted_train_dataset.data[train_dataset.targets==7, 25, 25] = 255\n",
    "tainted_test_dataset.data[test_dataset.targets==7, 25, 25] = 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.subplot(1,4,1)\n",
    "plt.imshow(tainted_train_dataset[3][0][0], cmap=plt.get_cmap('gray'))\n",
    "plt.subplot(1,4,2)\n",
    "plt.imshow(tainted_train_dataset[23][0][0], cmap=plt.get_cmap('gray'))\n",
    "plt.subplot(1,4,3)\n",
    "plt.imshow(tainted_train_dataset[15][0][0], cmap=plt.get_cmap('gray'))\n",
    "plt.subplot(1,4,4)\n",
    "plt.imshow(tainted_train_dataset[29][0][0], cmap=plt.get_cmap('gray'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><h4>\n",
    "Task 1.1: </h4>\n",
    "We have locally changed images of 7s artificially for this exercise. What are some examples of ways that images can be corrupted or tainted during real-life data colleciton, for example in a hospital imaging environment or microscopy lab?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Type answer here!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><h4>\n",
    "Task 1.2: </h4>\n",
    "In your above examples, if you knew you had a local corruption or difference between images in different classes of your data, could you remove it? How?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Type answer here!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1.2: Global Corrution of data\n",
    "\n",
    "Some data corruption or domain differences cover the whole image, rather than being localized to a specific location. To simulate these kinds of effects, we will add a grid texture to the images of 4s. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may have noticed that the images are stored as arrays of integers. First we cast them to float to be able to add textures easily without integer wrapping issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast to float\n",
    "tainted_train_dataset.data = tainted_train_dataset.data.type(torch.FloatTensor) \n",
    "tainted_test_dataset.data = tainted_test_dataset.data.type(torch.FloatTensor)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we create the grid texture and visualize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create grid texture\n",
    "texture = numpy.zeros(tainted_test_dataset.data.shape[1:])\n",
    "texture[::2,::2] = 80 \n",
    "texture = convolve(texture, weights=[[0.5,1,0.5],[1,0.1,0.5],[1,0.5,0]])\n",
    "texture = torch.from_numpy(texture)\n",
    "\n",
    "plt.imshow(texture, cmap=plt.get_cmap('gray'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we add the texture to all 4s in the train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Adding the texture to all images of 4's:\n",
    "tainted_train_dataset.data[train_dataset.targets==4] += texture\n",
    "tainted_test_dataset.data[test_dataset.targets==4] += texture"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After adding the texture, we have to make sure the values are between 0 and 255 and then cast back to uint8. \n",
    "Then we visualize a couple 4s from the dataset to see if the grid texture has been added properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clamp all images to avoid values above 255 that might occur:\n",
    "tainted_train_dataset.data = torch.clamp(tainted_train_dataset.data, 0, 255)\n",
    "tainted_test_dataset.data  = torch.clamp(tainted_test_dataset.data, 0, 255)\n",
    "\n",
    "# Cast back to byte:\n",
    "tainted_train_dataset.data = tainted_train_dataset.data.type(torch.uint8) \n",
    "tainted_test_dataset.data = tainted_test_dataset.data.type(torch.uint8) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize example 4s\n",
    "plt.subplot(1,4,1)\n",
    "plt.imshow(tainted_train_dataset[9][0][0], cmap=plt.get_cmap('gray'))\n",
    "plt.subplot(1,4,2)\n",
    "plt.imshow(tainted_train_dataset[26][0][0], cmap=plt.get_cmap('gray'))\n",
    "plt.subplot(1,4,3)\n",
    "plt.imshow(tainted_train_dataset[20][0][0], cmap=plt.get_cmap('gray'))\n",
    "plt.subplot(1,4,4)\n",
    "plt.imshow(tainted_train_dataset[53][0][0], cmap=plt.get_cmap('gray'))\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><h4>\n",
    "Task 1.4:</h4>\n",
    "Think of a realistic example of such a corruption that would affect only some classes of data. If you notice the differences between classes, could you remove it? How?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Type answer here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><h4>\n",
    "Task 1.5:</h4>\n",
    "Given the changes we made to generate the tainted dataset, do you think a digit classification network trained on the tainted data will converge? Are the classes more or less distinct from each other than in the untainted dataset?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Type answer here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\"><h3>\n",
    "    Checkpoint 1</h3>\n",
    "\n",
    "Post to Element when you have reached Checkpoint 1. We will discuss all the questions and make more predictions!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"><h3>\n",
    "    Bonus Questions:</h3>\n",
    "    Note that we only added the white dot to the images of 7s and the grid to images of 4s, not all classes.\n",
    "    <ol>\n",
    "        <li> Consider a dataset with white dots on images of all digits: let's call it the <b>all-dots</b> data. How different is this from the original dataset? Are the classes more or less distinct from each other? </li>\n",
    "        <li> How do you think a digit classifier trained on <b>all-dots</b> data and tested on <b>all-dots</b> data would perform? </li>\n",
    "        <li> Now consider the analagous <b>all-grid</b> data with the grid pattern added to all images. Are the classes more or less distinct from each other? Would a digit classifier trained on <b>all-grid</b> converge?</li>\n",
    "    </ol>\n",
    "If you want to test your hypotheses, you can create these all-dots and all-grid train and test datasets and use them for training in bonus questions of the following section.\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Create and Train an Image Classification Neural Network on Clean and Tainted Data\n",
    "\n",
    "From Part 1, we have a clean dataset and a dataset that has been tainted with effects that simulate local and global effects that could happen in real collection scenarios. Now we must create and train a neural network to classify the digits, so that we can examine what happens in each scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "    \n",
    "# Dense model:\n",
    "class DenseModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc0 = nn.Linear(784, 256)\n",
    "        self.fc1 = nn.Linear(256, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc0(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will train the neural network. A training function is provided below - this should be familiar, but make sure you look it over and understand what is happening in the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Training function:\n",
    "def train_mnist(model, train_loader, batch_size, criterion, optimizer, history):\n",
    "    model.train()\n",
    "    pbar = tqdm(total=len(tainted_train_dataset)//batch_size)\n",
    "    for batch_idx, (raw, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        raw = raw.cuda()\n",
    "        target = target.cuda()\n",
    "        output = model(raw)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        history.append(loss.item())\n",
    "        pbar.update(1)\n",
    "    return history"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to choose hyperparameters for our model. We have selected to train for two epochs, with a batch size of 64 for training and 1000 for testing. We are using the cross entropy loss, a standard multi-class classification loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch\n",
    "\n",
    "# Let's set some hyperparameters:\n",
    "n_epochs = 2\n",
    "batch_size_train = 64\n",
    "batch_size_test = 1000\n",
    "\n",
    "# Loss function:\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we initialize a clean model, and a tainted model. We want to have reproducible results, so we set the initial weights with a specific random seed. The seed number does not matter, just that it is the same!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the clean and tainted models\n",
    "model_clean = DenseModel().cuda()\n",
    "model_tainted = DenseModel().cuda()\n",
    "\n",
    "# Weight initialisation:\n",
    "def init_weights(m):\n",
    "    if isinstance(m, (nn.Linear, nn.Conv2d)):\n",
    "        torch.nn.init.xavier_uniform_(m.weight, )\n",
    "        m.bias.data.fill_(0.01)\n",
    "   \n",
    "# Fixing seed with magical number and setting weights:\n",
    "torch.random.manual_seed(42)\n",
    "model_clean.apply(init_weights)\n",
    "\n",
    "# Fixing seed with magical number and setting weights:\n",
    "torch.random.manual_seed(42)\n",
    "model_tainted.apply(init_weights)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we initialize the clean and tainted dataloaders, again with a specific random seed for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising dataloaders:\n",
    "train_loader_tainted = torch.utils.data.DataLoader(tainted_train_dataset,\n",
    "  batch_size=batch_size_train, shuffle=True, generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "  batch_size=batch_size_train, shuffle=True, generator=torch.Generator().manual_seed(42))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time to train the neural networks! We are storing the training loss history for each model so we can visualize it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We store history here:\n",
    "history = {\"loss_tainted\": [],\n",
    "           \"loss_clean\": []}\n",
    "\n",
    "# Training loop for clean model:\n",
    "for epoch in range(n_epochs):\n",
    "    train_mnist(model_clean,\n",
    "          train_loader,\n",
    "          batch_size_train,\n",
    "          criterion,\n",
    "          optim.Adam(model_clean.parameters(), lr=0.001),\n",
    "          history[\"loss_clean\"])\n",
    "    \n",
    "# Training loop for tainted model:\n",
    "for epoch in range(n_epochs):\n",
    "    train_mnist(model_tainted,\n",
    "          train_loader_tainted,\n",
    "          batch_size_train,\n",
    "          criterion,\n",
    "          optim.Adam(model_tainted.parameters(), lr=0.001),\n",
    "          history[\"loss_tainted\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we visualize the loss history for the clean and tainted models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the loss history:\n",
    "fig = plt.figure()\n",
    "plt.plot(history[\"loss_clean\"], color='blue')\n",
    "plt.plot(history[\"loss_tainted\"], color='red')\n",
    "plt.legend(['Train Loss Clean', \"Train Loss Tainted\"], loc='upper right')\n",
    "plt.xlabel('number of training examples seen')\n",
    "plt.ylabel('negative log likelihood loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><h4>\n",
    "Task 2.1:</h4>\n",
    "Why do you think the tainted network has lower training loss than the clean network?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Type answer here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><h4>\n",
    "Task 2.2:</h4>\n",
    "Do you think the tainted network will be more accurate than the clean network when applied to the <b>tainted</b> test data? Why?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Type answer here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><h4>\n",
    "Task 2.3:</h4>\n",
    "Do you think the tainted network will be more accurate than the clean network when applied to the <b>clean</b> test data? Why?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Type answer here!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\"><h3>\n",
    "    Checkpoint 2</h3>\n",
    "\n",
    "Post to Element when you have reached Checkpoint 2. We will discuss our predictions!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"><h3>\n",
    "    Bonus Questions:</h3>\n",
    "    <ol>\n",
    "        <li>Train a model on the <b>all-grid</b> training dataset from the bonus questions in Part 1. How does the  all-grid training loss compare to the clean and tainted models? Why?</li>\n",
    "        <li> How do you think a digit classifier trained on <b>all-grid</b> data and tested on <b>all-grid</b> data would perform? </li>\n",
    "        <li> What about a digit classifier trained on <b>all-grid</b> data and tested on <b>untainted</b> data? </li>\n",
    "    </ol>\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Examining the Results of the Clean and Tainted Networks\n",
    "\n",
    "Now that we have initialized our clean and tainted datasets and trained our models on them, it is time to examine how these models perform on the clean and tainted test sets!\n",
    "\n",
    "We provide a `predict` function below that will return the prediction and ground truth labels given a particualr model and dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# predict the test dataset\n",
    "def predict(model, dataset):\n",
    "    dataset_prediction = []\n",
    "    dataset_groundtruth = []\n",
    "    with torch.no_grad():\n",
    "        for x, y_true in dataset:\n",
    "            inp = x[None].cuda()\n",
    "            y_pred = model(inp)\n",
    "            dataset_prediction.append(y_pred.argmax().cpu().numpy())\n",
    "            dataset_groundtruth.append(y_true)\n",
    "    \n",
    "    return np.array(dataset_prediction), np.array(dataset_groundtruth)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we call the predict method with the clean and tainted models on the clean and tainted datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pred_clean_clean, true_labels = predict(model_clean, test_dataset)\n",
    "pred_clean_tainted, _ = predict(model_clean, tainted_test_dataset)\n",
    "pred_tainted_clean, _ = predict(model_tainted, test_dataset)\n",
    "pred_tainted_tainted, _ = predict(model_tainted, tainted_test_dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can investivate the results using the confusion matrix, which you should recall from the Introduction to Machine Learning exercise. The function in the cell below will create a nicely annotated confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "# Plot confusion matrix \n",
    "# orginally from Runqi Yang; \n",
    "# see https://gist.github.com/hitvoice/36cf44689065ca9b927431546381a3f7\n",
    "def cm_analysis(y_true, y_pred, title, figsize=(10,10)):\n",
    "    \"\"\"\n",
    "    Generate matrix plot of confusion matrix with pretty annotations.\n",
    "    The plot image is saved to disk.\n",
    "    args: \n",
    "      y_true:    true label of the data, with shape (nsamples,)\n",
    "      y_pred:    prediction of the data, with shape (nsamples,)\n",
    "      filename:  filename of figure file to save\n",
    "      labels:    string array, name the order of class labels in the confusion matrix.\n",
    "                 use `clf.classes_` if using scikit-learn models.\n",
    "                 with shape (nclass,).\n",
    "      ymap:      dict: any -> string, length == nclass.\n",
    "                 if not None, map the labels & ys to more understandable strings.\n",
    "                 Caution: original y_true, y_pred and labels must align.\n",
    "      figsize:   the size of the figure plotted.\n",
    "    \"\"\"\n",
    "    labels = [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"]\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    cm_sum = np.sum(cm, axis=1, keepdims=True)\n",
    "    cm_perc = cm / cm_sum.astype(float) * 100\n",
    "    annot = np.empty_like(cm).astype(str)\n",
    "    nrows, ncols = cm.shape\n",
    "    for i in range(nrows):\n",
    "        for j in range(ncols):\n",
    "            c = cm[i, j]\n",
    "            p = cm_perc[i, j]\n",
    "            if i == j:\n",
    "                s = cm_sum[i]\n",
    "                annot[i, j] = '%.1f%%\\n%d/%d' % (p, c, s)\n",
    "            elif c == 0:\n",
    "                annot[i, j] = ''\n",
    "            else:\n",
    "                annot[i, j] = '%.1f%%\\n%d' % (p, c)\n",
    "    cm = pd.DataFrame(cm, index=labels, columns=labels)\n",
    "    cm.index.name = 'Actual'\n",
    "    cm.columns.name = 'Predicted'\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    ax=sns.heatmap(cm, annot=annot, fmt='', vmax=30)\n",
    "    ax.set_title(title)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will generate confusion matrices for each model/data combination. Take your time and try and interpret these, and then try and answer the questions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_analysis(true_labels, pred_clean_clean, \"Clean Model on Clean Data\")\n",
    "cm_analysis(true_labels, pred_clean_tainted, \"Clean Model on Tainted Data\")\n",
    "cm_analysis(true_labels, pred_tainted_clean, \"Tainted Model on Clean Data\")\n",
    "cm_analysis(true_labels, pred_tainted_tainted, \"Tainted Model on Tainted Data\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"><h3>\n",
    "    Questions:</h3>\n",
    "    <ol>\n",
    "        <li>Which model/data combination had the best overall performance? Worst?</li>\n",
    "        <li>Which digits were most confused with each other? Did this change between tainted/clean models/datasets?</li>\n",
    "        <li>Why do you think the differences noted above happened?</li>\n",
    "    </ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><h4>\n",
    "Task 3.1:</h4>\n",
    "For the <b>clean</b> model and the <b>clean</b> dataset, which digit was least accurately predicted? What did the model predict instead? Why do you think these digits were confused by the model?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Type answer here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><h4>\n",
    "Task 3.2:</h4>\n",
    "Does the <b>tainted</b> model on the <b>tainted</b> dataset perform better or worse than the <b>clean</b> model on the <b>clean</b> dataset? Which digits is it better or worse on? Why do you think that is the case?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Type answer here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><h4>\n",
    "Task 3.3:</h4>\n",
    "For the <b>clean</b> model and the <b>tainted</b> dataset, was the local corruption on the 7s or the global corruption on the 4s harder for the model trained on clean data to deal with? Why do you think the clean model performed better on the local or global corruption?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Type answer here!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><h4>\n",
    "Task 3.4:</h4>\n",
    "Did the <b>tainted</b> model perform worse on <b>clean</b> 7s or <b>clean</b> 4s? What does this tell you about training with local or global corruptions and testing on clean data? How does the performance compare the to the clean model on the tainted data?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Type answer here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\"><h3>\n",
    "    Checkpoint 3</h3>\n",
    "\n",
    "Post to Element when you have reached Checkpoint 3, and will will discuss our results and reasoning about why they might have happened.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"><h3>\n",
    "    Bonus Questions:</h3>\n",
    "    <ol>\n",
    "        <li> Run predict with the model trained on the <b>all-grid</b> data using both the clean and all-grid testing data. Then generate the confusion matrices. </li>\n",
    "        <li> How does the <b>all-grid</b> model perform on <b>all-grid</b> data compared to the <b>clean</b> model on <b>clean</b> data? What about the <b>all-grid</b> model on <b>clean</b> data?</li>\n",
    "        <li> In a realistic situation, is it better to have corruption or noise on all your data, or just a subset of the classes? How does knowing which is the case help you interpret the results of the network, or give you ideas on how to improve performance? </li>\n",
    "    </ol>\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4: Interpretation with Integrated Gradients\n",
    "Perhaps you formed some hypotheses about why the clean and tainted models did better or worse on certain datasets in the previous section. Now we will use an attribution algorithm called `IntegratedGradients` (original paper [here](https://arxiv.org/pdf/1703.01365.pdf)) to learn more about the inner workings of each model. This algorithm analyses a specific image and class, and uses the gradients of the network to find the regions of the image that are most important for the classification. We will learn more about Integrated Gradients and its limitations in the Knowledge Extraction Lecture and Exercise."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Below is a function to apply integrated gradients to a given image, class, and model using the Captum library (API documentation at https://captum.ai/api/integrated_gradients.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from captum.attr import IntegratedGradients\n",
    "\n",
    "def apply_integrated_gradients(test_input, model):\n",
    "    # move the model to cpu\n",
    "    model.cpu()\n",
    "    \n",
    "    # initialize algorithm\n",
    "    algorithm = IntegratedGradients(model)\n",
    "\n",
    "    # clear the gradients from the model\n",
    "    model.zero_grad()\n",
    "\n",
    "    # Get input and target tensors from test_input\n",
    "    input_tensor = test_input[0].unsqueeze(0)\n",
    "    input_tensor.requires_grad = True\n",
    "    target = test_input[1]\n",
    "\n",
    "    # Run attribution:\n",
    "    attributions = algorithm.attribute(\n",
    "        input_tensor,\n",
    "        target=target,\n",
    "        baselines=input_tensor * 0\n",
    "    )\n",
    "\n",
    "    return attributions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we provide a function to visualize the output of integrated gradients, using the function above to actually run the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from captum.attr import visualization as viz\n",
    "\n",
    "def visualize_integrated_gradients(test_input, model, plot_title):\n",
    "    attr_ig = apply_integrated_gradients(test_input, model)\n",
    "\n",
    "    # Transpose integrated gradients output\n",
    "    attr_ig = np.transpose(attr_ig[0].cpu().detach().numpy(), (1, 2, 0))\n",
    "    \n",
    "    # Transpose and normalize original image:\n",
    "    original_image = np.transpose((test_input[0].detach().numpy() * 0.5) + 0.5, (1, 2, 0))\n",
    "\n",
    "    # This visualises the attribution of labels to pixels\n",
    "    viz.visualize_image_attr(attr_ig, \n",
    "                             original_image, \n",
    "                             method=\"blended_heat_map\",\n",
    "                             sign=\"absolute_value\",\n",
    "                             show_colorbar=True, \n",
    "                             title=plot_title,\n",
    "                             fig_size=(3,3))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To examine the results, we we will call the `visualize_integrated_gradients` with the tainted and clean models on the tainted and clean sevens and fours. We start with the tainted model on a tainted seven. The visualization will show the original image plus an overlay that signifies the importance of each pixel. The more intense the value of the overaly, the more important the pixel. We can see that the tainted model focuses its attention on the added white pixel in the bottom white corner to identify sevens, ignoring all the other pixels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "visualize_integrated_gradients(tainted_test_dataset[0], model_tainted, \"Tainted Model on Tainted 7\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now examine the visualizations for the tainted and clean models on other tainted and clean digits and use them to answer the questions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_integrated_gradients(tainted_test_dataset[0], model_clean, \"Clean Model on Tainted 7\")\n",
    "visualize_integrated_gradients(test_dataset[0], model_tainted, \"Tainted Model on Clean 7\")\n",
    "visualize_integrated_gradients(test_dataset[0], model_clean, \"Clean Model on Clean 7\")\n",
    "visualize_integrated_gradients(tainted_test_dataset[6], model_tainted, \"Tainted Model on Tainted 4\")\n",
    "visualize_integrated_gradients(tainted_test_dataset[6], model_clean, \"Clean Model on Tainted 4\")\n",
    "visualize_integrated_gradients(test_dataset[6], model_tainted, \"Tainted Model on Clean 4\")\n",
    "visualize_integrated_gradients(test_dataset[6], model_clean, \"Clean Model on Clean 4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><h4>\n",
    "    Task 4.1: Interpereting the focus on 7s</h4>\n",
    "Where did the <b>tainted</b> model focus its attention for the tainted and clean 7s? How was the focus different for the clean model on the tainted and clean 7s? How does the focus of the tainted model explain why it would perform poorly at predicting clean 7s?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Type answer here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><h4>\n",
    "    Task 4.2: Interpereting the focus on 4s</h4>\n",
    "Where did the <b>tainted</b> model focus its attention for the tainted and clean 4s? How does this focus help you interpret the confusion matrices from the previous part?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Type answer here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><h4>\n",
    "    Task 4.3: Reflecting on Integrated Gradients</h4>\n",
    "Did you find the integrated gradients more useful for the global or local corruptions of the data? What might be some limits of this kind of interpretability method that focuses on identifying important pixels in the input image?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Type answer here!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"><h3>\n",
    "    Checkpoint 4</h3>\n",
    "    <ol>\n",
    "        Congrats on finishing the intergrated gradients task! Let us know on Element that you reached checkpoint 4, and feel free to look at other interpretability methods in the Captum library if you're interested.\n",
    "    </ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"><h3>\n",
    "    Bonus Questions</h3>\n",
    "    <ol>\n",
    "        <li>Run integrated gradients on the <b>all-grid</b> model and clean and all-grid examples. Did the model learn to ignore the grid pattern for the all-grid test set? What happens when the grid pattern is missing in the clean data? </li>\n",
    "        <li>Where did the \"clean\" model focus its attention on for the tainted \"4\" and \"7\"?</li>\n",
    "        <li>How do these results help you interpret the confusion matrices? Were your predictions correct about why certain models did better or worse on certain digits?</li>\n",
    "    </ol>\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Importance of using the right training data\n",
    "\n",
    "Now we will move on from image classification to denoising, and show why it is particularly important to ensure that your training and test data are from the same distribution for these kinds of networks.\n",
    "\n",
    "For this exercise, we will first train a simple CNN model to denoise MNIST images of digits, and then apply it to the Fashion MNIST to see what happens when the training and inference data are mismatched.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will write a function to add noise to the MNIST dataset, so that we can train a model to denoise it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# A simple function to add noise to tensors:\n",
    "def add_noise(tensor, power=1.5):\n",
    "    return tensor * torch.rand(tensor.size()).to(tensor.device) ** power + 0.75*torch.randn(tensor.size()).to(tensor.device)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will visualize a couple MNIST examples with and without noise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Let's visualise MNIST images with noise:\n",
    "def show(index):\n",
    "    plt.subplot(1,4,1)\n",
    "    plt.imshow(train_dataset[index][0][0], cmap=plt.get_cmap('gray'))\n",
    "    plt.subplot(1,4,2)\n",
    "    plt.imshow(add_noise(train_dataset[index][0][0]), cmap=plt.get_cmap('gray'))\n",
    "    plt.subplot(1,4,3)\n",
    "    plt.imshow(train_dataset[index+1][0][0], cmap=plt.get_cmap('gray'))\n",
    "    plt.subplot(1,4,4)\n",
    "    plt.imshow(add_noise(train_dataset[index+1][0][0]), cmap=plt.get_cmap('gray'))\n",
    "    plt.show()\n",
    "\n",
    "# We pick 8 images to show:\n",
    "for i in range(8):\n",
    "    show(123*i)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UNet model\n",
    "\n",
    "Let's try denoising with a UNet, \"CARE-style\". As UNets and denoising implementations are not the focus of this exercise, we provide the model for you in the following cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from https://discuss.pytorch.org/t/unet-implementation/426\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels=1,\n",
    "        n_classes=1,\n",
    "        depth=3,\n",
    "        wf=4,\n",
    "        padding=True,\n",
    "        batch_norm=False,\n",
    "        up_mode='upsample',\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Implementation of\n",
    "        U-Net: Convolutional Networks for Biomedical Image Segmentation\n",
    "        (Ronneberger et al., 2015)\n",
    "        https://arxiv.org/abs/1505.04597\n",
    "        Using the default arguments will yield the exact version used\n",
    "        in the original paper\n",
    "        Args:\n",
    "            in_channels (int): number of input channels\n",
    "            n_classes (int): number of output channels\n",
    "            depth (int): depth of the network\n",
    "            wf (int): number of filters in the first layer is 2**wf\n",
    "            padding (bool): if True, apply padding such that the input shape\n",
    "                            is the same as the output.\n",
    "                            This may introduce artifacts\n",
    "            batch_norm (bool): Use BatchNorm after layers with an\n",
    "                               activation function\n",
    "            up_mode (str): one of 'upconv' or 'upsample'.\n",
    "                           'upconv' will use transposed convolutions for\n",
    "                           learned upsampling.\n",
    "                           'upsample' will use bilinear upsampling.\n",
    "        \"\"\"\n",
    "        super(UNet, self).__init__()\n",
    "        assert up_mode in ('upconv', 'upsample')\n",
    "        self.padding = padding\n",
    "        self.depth = depth\n",
    "        prev_channels = in_channels\n",
    "        self.down_path = nn.ModuleList()\n",
    "        for i in range(depth):\n",
    "            self.down_path.append(\n",
    "                UNetConvBlock(prev_channels, 2 ** (wf + i), padding, batch_norm)\n",
    "            )\n",
    "            prev_channels = 2 ** (wf + i)\n",
    "\n",
    "        self.up_path = nn.ModuleList()\n",
    "        for i in reversed(range(depth - 1)):\n",
    "            self.up_path.append(\n",
    "                UNetUpBlock(prev_channels, 2 ** (wf + i), up_mode, padding, batch_norm)\n",
    "            )\n",
    "            prev_channels = 2 ** (wf + i)\n",
    "\n",
    "        self.last = nn.Conv2d(prev_channels, n_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        blocks = []\n",
    "        for i, down in enumerate(self.down_path):\n",
    "            x = down(x)\n",
    "            if i != len(self.down_path) - 1:\n",
    "                blocks.append(x)\n",
    "                x = F.max_pool2d(x, 2)\n",
    "\n",
    "        for i, up in enumerate(self.up_path):\n",
    "            x = up(x, blocks[-i - 1])\n",
    "\n",
    "        return self.last(x)\n",
    "\n",
    "\n",
    "class UNetConvBlock(nn.Module):\n",
    "    def __init__(self, in_size, out_size, padding, batch_norm):\n",
    "        super(UNetConvBlock, self).__init__()\n",
    "        block = []\n",
    "\n",
    "        block.append(nn.Conv2d(in_size, out_size, kernel_size=3, padding=int(padding)))\n",
    "        block.append(nn.ReLU())\n",
    "        if batch_norm:\n",
    "            block.append(nn.BatchNorm2d(out_size))\n",
    "\n",
    "        block.append(nn.Conv2d(out_size, out_size, kernel_size=3, padding=int(padding)))\n",
    "        block.append(nn.ReLU())\n",
    "        if batch_norm:\n",
    "            block.append(nn.BatchNorm2d(out_size))\n",
    "\n",
    "        self.block = nn.Sequential(*block)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.block(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "class UNetUpBlock(nn.Module):\n",
    "    def __init__(self, in_size, out_size, up_mode, padding, batch_norm):\n",
    "        super(UNetUpBlock, self).__init__()\n",
    "        if up_mode == 'upconv':\n",
    "            self.up = nn.ConvTranspose2d(in_size, out_size, kernel_size=2, stride=2)\n",
    "        elif up_mode == 'upsample':\n",
    "            self.up = nn.Sequential(\n",
    "                nn.Upsample(mode='bilinear', scale_factor=2),\n",
    "                nn.Conv2d(in_size, out_size, kernel_size=1),\n",
    "            )\n",
    "\n",
    "        self.conv_block = UNetConvBlock(in_size, out_size, padding, batch_norm)\n",
    "\n",
    "    def center_crop(self, layer, target_size):\n",
    "        _, _, layer_height, layer_width = layer.size()\n",
    "        diff_y = (layer_height - target_size[0]) // 2\n",
    "        diff_x = (layer_width - target_size[1]) // 2\n",
    "        return layer[\n",
    "            :, :, diff_y : (diff_y + target_size[0]), diff_x : (diff_x + target_size[1])\n",
    "        ]\n",
    "\n",
    "    def forward(self, x, bridge):\n",
    "        up = self.up(x)\n",
    "        crop1 = self.center_crop(bridge, up.shape[2:])\n",
    "        out = torch.cat([up, crop1], 1)\n",
    "        out = self.conv_block(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training loop code is also provided here. It is similar to the code used to train the image classification model previously, but look it over to make sure there are no surprises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def train_denoising_model(train_loader, model, criterion, optimizer, history):\n",
    "    \n",
    "    # Puts model in 'training' mode:\n",
    "    model.train()\n",
    "    \n",
    "    # Initialises progress bar:\n",
    "    pbar = tqdm(total=len(train_loader)//batch_size_train)\n",
    "    for batch_idx, (image, target) in enumerate(train_loader):\n",
    "\n",
    "        # add line here during Task 2.2\n",
    "        \n",
    "        # Zeroing gradients:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Moves image to GPU memory:\n",
    "        image = image.cuda()\n",
    "        \n",
    "        # Adds noise to make the noisy image:\n",
    "        noisy = add_noise(image)\n",
    "        \n",
    "        # Runs model on noisy image:\n",
    "        output = model(noisy)\n",
    "        \n",
    "        # Computes loss:\n",
    "        loss = criterion(output, image)\n",
    "        \n",
    "        # Backpropagates gradients:\n",
    "        loss.backward()\n",
    "        \n",
    "        # Optimises model parameters given the current gradients:\n",
    "        optimizer.step()\n",
    "        \n",
    "        # appends loss history:\n",
    "        history[\"loss\"].append(loss.item())\n",
    "        \n",
    "        # updates progress bar:\n",
    "        pbar.update(1)\n",
    "    return history"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we choose hyperparameters and initialize the model and data loaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch\n",
    "\n",
    "# Some hyper-parameters:\n",
    "n_epochs = 5\n",
    "batch_size_train = 64\n",
    "batch_size_test = 1000\n",
    "\n",
    "# Dictionary to store loss history:\n",
    "history = {\"loss\": []}\n",
    "\n",
    "# Model:\n",
    "unet_model = UNet().cuda()\n",
    "\n",
    "# Loss function:\n",
    "criterion = F.mse_loss #mse_loss\n",
    "\n",
    "# Optimiser:\n",
    "optimizer = optim.Adam(unet_model.parameters(), lr=0.0005)\n",
    "\n",
    "# Test loader:\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset,\n",
    "  batch_size=batch_size_test, shuffle=True)\n",
    "\n",
    "# Train loader:\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "  batch_size=batch_size_train, shuffle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we run the training loop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop:\n",
    "for epoch in range(n_epochs):\n",
    "    train_denoising_model(train_dataset, unet_model, criterion, optimizer, history)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we will visualize the training loss. If all went correctly, it should decrease from around 1.0 to less than 0.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Visualization\n",
    "fig = plt.figure()\n",
    "plt.plot(history[\"loss\"], color='blue')\n",
    "plt.legend(['Train Loss'], loc='upper right')\n",
    "plt.xlabel('number of training examples seen')\n",
    "plt.ylabel('mean squared error loss')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check denoising performance\n",
    "\n",
    "We see that the training loss decreased, but let's apply the model to the test set to see how well it was able to recover the digits from the noisy images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_denoising(image, model):\n",
    "    # add batch and channel dimensions\n",
    "    image = torch.unsqueeze(torch.unsqueeze(image, 0), 0)\n",
    "    prediction = model(image.cuda())\n",
    "    # remove batch and channel dimensions before returning\n",
    "    return prediction.detach().cpu()[0,0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displays: ground truth, noisy, and denoised images\n",
    "def visualize_denoising(model, dataset, index):\n",
    "    orig_image = dataset[index][0][0]\n",
    "    noisy_image = add_noise(orig_image)\n",
    "    denoised_image = apply_denoising(noisy_image, model)\n",
    "    plt.subplot(1,4,1)\n",
    "    plt.imshow(orig_image, cmap=plt.get_cmap('gray'))\n",
    "    plt.subplot(1,4,2)\n",
    "    plt.imshow(noisy_image, cmap=plt.get_cmap('gray'))\n",
    "    plt.subplot(1,4,3)\n",
    "    plt.imshow(denoised_image, cmap=plt.get_cmap('gray'))\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# We pick 8 images to show:\n",
    "for i in range(8):\n",
    "    visualize_denoising(unet_model, test_dataset, 123*i)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><h4>\n",
    "    Task 5.1: </h4>\n",
    "Did the denoising net trained on MNIST work well on unseen test data? What do you think will happen when we apply it to the Fashion-MNIST data?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Type answer here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply trained model on 'wrong' data \n",
    "\n",
    "Apply the denoising model trained above to some example _noisy_ images derived from the Fashion-MNIST dataset.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Fashion MNIST dataset\n",
    "\n",
    "Similar to the regular MNIST, we will use the pytorch FashionMNIST dataset. This was downloaded in the setup.sh script, so here we are just loading it into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm_train_dataset = torchvision.datasets.FashionMNIST('./fashion_mnist', train=True, download=False,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ]))\n",
    "\n",
    "fm_test_dataset = torchvision.datasets.FashionMNIST('./fashion_mnist', train=False, download=False,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we apply the denoising model we trained on the MNIST data to FashionMNIST, and visualize the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(8):\n",
    "    visualize_denoising(unet_model, fm_train_dataset, 123*i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><h4>\n",
    "    Task 5.2: </h4>\n",
    "What happened when the MNIST denoising model was applied to the FashionMNIST data? Why do you think the results look as they do?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Type answer here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><h4>\n",
    "    Task 5.3: </h4>\n",
    "Can you imagine any real-world scenarios where a denoising model would change the content of an image?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Type answer here!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"><h3>\n",
    "    Checkpoint 5</h3>\n",
    "    <ol>\n",
    "        Congrats on reaching the final checkpoint! Let us know on Element, and we'll discuss the questions once reaching critical mass.\n",
    "    </ol>\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"><h3>\n",
    "    Bonus Questions</h3>\n",
    "    <ol>\n",
    "        <li>Try training a FashionMNIST denoising network and applying it to MNIST. Or, try training a denoising network on both datasets and see how it works on each.</li>\n",
    "        <li>Go back to Part 4 and try another attribution method, such as <a href=\"https://captum.ai/api/saliency.html\">Saliency</a>, and see how the results differ.</li>\n",
    "    </ol>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
